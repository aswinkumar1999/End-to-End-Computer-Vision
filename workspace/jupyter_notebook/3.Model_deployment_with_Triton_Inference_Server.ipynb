{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8439ea6f-1733-4d22-bd4d-19c193ddea84",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../Start_here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"2.Object_detection_using_TAO_YOLOv4.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 34%; text-align: center;\">\n",
    "        <a href=\"1.Data_labeling_and_preprocessing.ipynb\">1</a>\n",
    "        <a href=\"2.Object_detection_using_TAO_YOLOv4.ipynb\">2</a>\n",
    "        <a >3</a>\n",
    "        <a href=\"4.Model_deployment_with_DeepStream.ipynb\">4</a>\n",
    "        <a href=\"5.Measure_object_size_using_OpenCV.ipynb\">5</a>\n",
    "        <a href=\"6.Challenge_DeepStream.ipynb\">6</a>\n",
    "        <a href=\"7.Challenge_Triton.ipynb\">7</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"4.Model_deployment_with_DeepStream.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47202426-f160-4c29-a8f6-063282168535",
   "metadata": {},
   "source": [
    "# Model deployment with Triton Inference Server\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9e259-6801-48b2-a0be-9a87b5bcb8a1",
   "metadata": {},
   "source": [
    "**The goal of this notebook is to make you understand how to:**\n",
    "\n",
    "- Deploy a NVIDIA® TAO Toolkit model to NVIDIA Triton™ Inference Server\n",
    "- Send inference requests to the server\n",
    "- Process and render the outputs in a useful format\n",
    "- Monitor server performance\n",
    "- Maximize throughput and utilization with inference optimization\n",
    "\n",
    "**Contents of this notebook:**\n",
    "\n",
    "- [Triton Inference Server](#Triton-Inference-Server)\n",
    "- [Setup server and client](#Setup-server-and-client)\n",
    "- [Create the model repository](#Create-the-model-repository)\n",
    "- [Create configuration file](#Create-configuration-file)\n",
    "- [Check loaded model in Triton Inference Server](#Check-loaded-model-in-Triton-Inference-Server)\n",
    "- [Send inference request to the server](#Send-inference-request-to-the-server)\n",
    "- [Performance analyzer](#Performance-analyzer)\n",
    "- [Improve inference performance](#Improve-inference-performance)\n",
    "    - [Variable batch size](#Variable-batch-size)\n",
    "    - [Dynamic batching](#Dynamic-batching)\n",
    "    - [Asynchronous inference](#Asynchronous-inference)\n",
    "    - [gRPC protocol](#gRPC-protocol)\n",
    "- [Analyze the impact of inference optimization](#Analyze-the-impact-of-inference-optimization)\n",
    "- [Additional resources](#Additional-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6482e78f-69b0-4969-8e65-fc6fe176adfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Triton Inference Server\n",
    "\n",
    "Triton Inference Server, part of the NVIDIA AI platform, is an open-source inference serving software that simplifies the deployment of AI models at scale in production. It supports all major frameworks (such as TensorFlow, PyTorch, MXNet, ONNX, TensorRT, RAPIDS FIL, and more) and enables AI developers and researchers to easily deploy, run, and scale their trained AI models on any GPU- or CPU-based infrastructure, cloud, data center, or edge. Triton offers features like dynamic batching, concurrent execution, optimal model configuration, model ensemble, and streaming inputs to maximize throughput and utilization. To get started quickly with Triton Inference Server, a ready-to-use container can be downloaded from the NVIDIA NGC catalog.\n",
    "\n",
    "The below figure shows the Triton Inference Server high-level workflow, so let's briefly analyze it. The models that Triton makes available for inferencing are stored in the model repository, a file-system-based persistent volume. Inference requests arrive at the server via standard HTTP or gRPC interface and are then routed to the appropriate per-model scheduler. Triton implements multiple scheduling and batching algorithms that can be configured differently depending on the model. Each scheduler optionally performs batching of inference requests and then passes them to the backend corresponding to the model type, which performs inferencing and produces the requested outputs. Under the hood, Triton executes multiple models from the same or different frameworks concurrently on a single GPU or CPU, handles multi-GPU servers by creating an instance of each model on each GPU, loads and unloads models into and out of the inference server based on changes to fit in GPU or CPU memory.\n",
    "\n",
    "<img src=\"images/triton_inference_server.jpg\" width=\"720\">\n",
    "<div style=\"font-size:11px\">Source: https://developer.nvidia.com/nvidia-triton-inference-server</div><br>\n",
    "\n",
    "In this notebook, we will take our TAO YOLOv4 model, already converted into a TensorRT engine representation, and deploy it into Triton Inference Server. In the first part, we will see how to create a model repository, a configuration file, and how to send inference requests to the deployed model. In the second part, we will see how to monitor the server performance and some tricks to further speed up inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4cd161-1c41-4322-9cc7-6693c04eb8f5",
   "metadata": {},
   "source": [
    "## Setup server and client\n",
    "\n",
    "**Server**\n",
    "\n",
    "To successfully execute the code in this notebook, you should already have an instance of Triton Inference Server running. This consists of the Triton NGC container that can be found [here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver). The code to run a Triton Server instance with Docker is shown below and more details can be found in the [quickstart](https://github.com/triton-inference-server/server/blob/r22.07/docs/quickstart.md) and [build](https://github.com/triton-inference-server/server/blob/r22.07/docs/build.md) instructions in the Triton documentation on the GitHub page.\n",
    "\n",
    "In this case, the container is used in polling mode, meaning changes we make to the model repository while running the code cells will be detected periodically and Triton will attempt to load and unload models as necessary based on those changes. \n",
    "\n",
    "```\n",
    "docker run \\\n",
    "  --gpus=1 --rm \\\n",
    "  -p 8000:8000 -p 8001:8001 -p 8002:8002 \\\n",
    "  -v /full/path/to/model/repository:/models \\\n",
    "  nvcr.io/nvidia/tritonserver:<yy.mm>-py3 \\\n",
    "  tritonserver \\\n",
    "  --model-repository=/models \\\n",
    "  --exit-on-error=false \\\n",
    "  --model-control-mode=poll \\\n",
    "  --repository-poll-secs 30\n",
    "```\n",
    "\n",
    "The `--gpus=1` flag indicates that 1 system GPU should be made available to Triton for inferencing, while `<yy.mm>` is the version of Triton that you want to use and pull from the NVIDIA Container Toolkit. The path to the model repository needs to be set as well.\n",
    "\n",
    "**Client**\n",
    "\n",
    "The Triton client libraries that provide application programming interfaces (APIs) that make it easy to communicate with Triton from a C++ or Python application have also been installed. Using these libraries we can send requests to Triton to access all its functionalities: inferencing, status and health, statistics and metrics, model repository management, etc.\n",
    "\n",
    "The easiest way to get the Python client library is to pip install the `tritonclient` module, as detailed below. For more details on how to download or build the client libraries, you can find their documentation [here](https://github.com/triton-inference-server/client).\n",
    "\n",
    "```\n",
    "pip install nvidia-pyindex\n",
    "pip install tritonclient[all]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0889894d-1094-45b3-b4a4-492f861ed642",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the model repository\n",
    "\n",
    "Triton Inference Server stores available models in the model repository. The directory where the models reside inside the container is specified when starting the server instance using the `tritonserver --model-repository=/models` flag. Each model then resides in its own subdirectory within the main model repository (i.e. each directory within `/models` represents a unique model). For example, in this notebook, we will deploy the TensorRT engine generated from the TAO training in the `yolov4_tao` subdirectory.\n",
    "\n",
    "The directories and files that compose a model repository must follow a required layout. Within each of these directories, there is a configuration file `config.pbtxt` that details information about the model like batch size, input shapes, deployment backend (PyTorch, ONNX, TensorFlow, TensorRT, etc.), and more. We will write our configuration file later in this notebook.\n",
    "\n",
    "Additionally, we can create one or more versions of our model that can coexist on the server. Each version lives under a subdirectory named with the respective version number, starting with `1`. It is within this subdirectory that our model file resides (e.g. `model.plan` is the default name for a TensorRT engine).\n",
    "\n",
    "To sum up, the layout of a minimal model repository should look like this:\n",
    "\n",
    "```\n",
    "models\n",
    "└── yolov4_tao\n",
    "    ├── 1\n",
    "    │   └── model.plan\n",
    "    └── config.pbtxt\n",
    "```\n",
    "\n",
    "For more details on how to work with model repositories and model directory structures in Triton Inference Server, please check the documentation [here](https://github.com/triton-inference-server/server/blob/r22.07/docs/model_repository.md).\n",
    "\n",
    "Below, we'll create the model directory structure for our TensorRT model and copy the engine we generated in the previous [2.Object_detection_using_TAO_YOLOv4.ipynb](2.Object_detection_using_TAO_YOLOv4.ipynb) notebook to the newly prepared folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d30ce-7859-4de2-b011-b78e77984232",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models/yolov4_tao/1/\n",
    "# Copy the TensorRT engine and rename it to match the default name model.plan\n",
    "!cp ../yolo_v4/export/trt.engine ../models/yolov4_tao/1/model.plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de33bbff-1392-44ab-a574-39842c24e4b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create configuration file\n",
    "\n",
    "With our TAO model already defined and exported in TensorRT plan representation, we now focus on creating the configuration file that provides required and optional information about the model.\n",
    "\n",
    "A minimal model configuration must specify the platform and/or backend properties, the max_batch_size property, and the input and output tensors of the model (name, data type, and shape). As you can see in the sample, a YOLOv4 model has 1 input node `Input` and 4 output nodes `BatchedNMS`, `BatchedNMS_1`, `BatchedNMS_2` and `BatchedNMS_3`.\n",
    "\n",
    "For more details on how to create model configuration files within Triton Inference Server, please see the documentation [here](https://github.com/triton-inference-server/server/blob/r22.07/docs/model_configuration.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6390ab0c-3d67-4619-a79e-73f6263c339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 16\n",
    "input [\n",
    "  {\n",
    "    name: \"Input\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 384, 640 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"BatchedNMS\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"BatchedNMS_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 200, 4 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"BatchedNMS_2\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 200 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"BatchedNMS_3\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 200 ]\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "with open(\"../models/yolov4_tao/config.pbtxt\", 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f136a4-40f7-4c14-9a76-0b6de9605d7c",
   "metadata": {},
   "source": [
    "## Check loaded model in Triton Inference Server\n",
    "\n",
    "With the model repository created, the TensorRT model defined and exported, and the configuration file written, we will now wait for Triton Inference Server to load our model. This notebook is set to continuously poll for modifications once every 30 seconds, so please run the cell below to ensure enough time has passed before proceeding (15 seconds have been added just to be safe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee54680-bcac-470a-9c6c-01f9f999ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleep 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85d3ee4-4946-4ee2-a35c-575c7209d5f6",
   "metadata": {},
   "source": [
    "At this point, our model should be deployed and ready to use! To confirm Triton Inference Server is up and running, we can see the output of a `curl` request to the below URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b8ccfa-a48e-40f9-9cf1-09746755a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v localhost:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4f353-ee11-4be3-8267-53fa09154804",
   "metadata": {},
   "source": [
    "The HTTP request returns status 200 if Triton is ready and non-200 if it is not ready.\n",
    "\n",
    "We can also send a `curl` request to our model endpoints to confirm our model is deployed and ready to use. This `curl` request returns status 200 if the model is ready and non-200 if it is not ready. \n",
    "\n",
    "Additionally, we will also see information about our model such as:\n",
    "- The name of our model.\n",
    "- The versions available for our model.\n",
    "- The backend platform (e.g. tensorrt_plan).\n",
    "- The inputs and outputs, with their respective names, data types, and shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b681f9-855b-4741-b8d3-0dc46776aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v localhost:8000/v2/models/yolov4_tao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff35a6c-c9ba-42d8-9df1-88ae68637c6a",
   "metadata": {},
   "source": [
    "## Send inference request to the server\n",
    "\n",
    "With our model deployed and ready to use, it is now time to send inference requests to it. We'll start by loading the `tritonclient.http` module and defining a set of variables including the name of our model, the URL where it is deployed, the model version, and paths from which to load the images and where to save the processed outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719d0603-4989-4fc1-9df4-2d0f7b8e73ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "sys.path.append(\"../source_code/N3\")\n",
    "from utils import convert_http_metadata_config\n",
    "\n",
    "verbose = False\n",
    "url = 'localhost:8000'\n",
    "model_name = 'yolov4_tao'\n",
    "model_version = '1'\n",
    "protocol = 'http'\n",
    "batch_size = 1\n",
    "image_filename = \"../data/testing/image_2/\"\n",
    "output_path = \"../source_code/N3/triton_output_http\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98e804c-81e2-40be-b7bf-eba12515f918",
   "metadata": {},
   "source": [
    "Then, we instantiate the Triton Client and get access to additional properties from the model metadata and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adf155a-43b6-454b-b48f-6f7245b2ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running the inference client \\n\")\n",
    "\n",
    "try:\n",
    "    triton_client = httpclient.InferenceServerClient(\n",
    "        url=url, verbose=verbose)\n",
    "except Exception as e:\n",
    "    print(\"client creation failed: \" + str(e))\n",
    "    sys.exit(1)\n",
    "\n",
    "# Make sure the model matches our requirements, and get some\n",
    "# properties of the model that we need for preprocessing\n",
    "try:\n",
    "    model_metadata = triton_client.get_model_metadata(\n",
    "        model_name=model_name, model_version=model_version)\n",
    "except InferenceServerException as e:\n",
    "    print(\"failed to retrieve the metadata: \" + str(e))\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    model_config = triton_client.get_model_config(\n",
    "        model_name=model_name, model_version=model_version)\n",
    "except InferenceServerException as e:\n",
    "    print(\"failed to retrieve the config: \" + str(e))\n",
    "    sys.exit(1)\n",
    "\n",
    "model_metadata, model_config = convert_http_metadata_config(\n",
    "    model_metadata, model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18cbfb1-65d5-4cad-8287-ea36057c0332",
   "metadata": {},
   "source": [
    "Next, we load the model and process the images from our input directory by converting, resizing, and loading them into a data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811cb04-8512-418e-aaf6-c1ec0c1d5e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov4_model import YOLOv4Model\n",
    "from tritonclient.utils import triton_to_np_dtype\n",
    "import os\n",
    "from frame import Frame\n",
    "\n",
    "triton_model = YOLOv4Model.from_metadata(model_metadata, model_config)\n",
    "max_batch_size = triton_model.max_batch_size\n",
    "target_shape = (triton_model.c, triton_model.h, triton_model.w)\n",
    "npdtype = triton_to_np_dtype(triton_model.triton_dtype)\n",
    "\n",
    "print(\"\\nLoading images... \\n\")\n",
    "\n",
    "frames = []\n",
    "\n",
    "if os.path.exists(image_filename):\n",
    "    # The input is a folder of images\n",
    "    if os.path.isdir(image_filename):\n",
    "        frames = [\n",
    "            Frame(os.path.join(image_filename, f),\n",
    "                triton_model.data_format,\n",
    "                npdtype,\n",
    "                target_shape)\n",
    "            for f in os.listdir(image_filename)\n",
    "            if os.path.isfile(os.path.join(image_filename, f)) and\n",
    "            os.path.splitext(f)[-1] in [\".jpg\", \".jpeg\", \".png\"]\n",
    "        ]\n",
    "    # The input is an image\n",
    "    else:\n",
    "        frames = [\n",
    "            Frame(os.path.join(image_filename),\n",
    "                triton_model.data_format,\n",
    "                npdtype,\n",
    "                target_shape)\n",
    "        ]\n",
    "    print(\"Done! \\n\")\n",
    "else:\n",
    "    print(\"No images found, please specify a valid path \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f348c583-ac03-404a-9c6b-37b0622b5600",
   "metadata": {},
   "source": [
    "Finally, we use a request generator to submit our inputs to the Triton Inference Server using the `triton_client.infer()` method, specifying our model name, version, inputs and outputs. The responses we get are stored in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f37d2-11fa-4c0b-9fb8-e74ffaca4ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from utils import requestGenerator\n",
    "import time\n",
    "\n",
    "# Send requests of batch_size images. If the number of\n",
    "# images isn't an exact multiple of batch_size then just\n",
    "# start over with the first images until the batch is filled.\n",
    "\n",
    "print(\"Sending inference request for batches of data \\n\")\n",
    "\n",
    "responses = []\n",
    "image_idx = 0\n",
    "last_request = False\n",
    "sent_count = 0\n",
    "pbar_total = len(frames)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=pbar_total) as pbar:\n",
    "    while not last_request:\n",
    "        batched_image_data = None\n",
    "\n",
    "        repeated_image_data = []\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            frame = frames[image_idx]\n",
    "\n",
    "            img = frame._load_img()\n",
    "            repeated_image_data.append(img)\n",
    "\n",
    "            image_idx = (image_idx + 1) % len(frames)\n",
    "            if image_idx == 0:\n",
    "                last_request = True\n",
    "\n",
    "        if max_batch_size > 0:\n",
    "            batched_image_data = np.stack(repeated_image_data, axis=0)\n",
    "        else:\n",
    "            batched_image_data = repeated_image_data[0]\n",
    "\n",
    "        # Send request\n",
    "        try:\n",
    "            req_gen_args = [batched_image_data, triton_model.input_names,\n",
    "                triton_model.output_names, triton_model.triton_dtype,\n",
    "                protocol.lower()]\n",
    "            req_generator = requestGenerator(*req_gen_args)\n",
    "            for inputs, outputs in req_generator:\n",
    "                sent_count += 1\n",
    "\n",
    "                responses.append(\n",
    "                    triton_client.infer(model_name,\n",
    "                                        inputs,\n",
    "                                        request_id=str(sent_count),\n",
    "                                        model_version=model_version,\n",
    "                                        outputs=outputs))\n",
    "\n",
    "        except InferenceServerException as e:\n",
    "            print(\"inference failed: \" + str(e))\n",
    "            sys.exit(1)\n",
    "        \n",
    "        pbar.update(batch_size)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Average latency: ~{} seconds\".format((end_time - start_time) / sent_count))\n",
    "print(\"Average throughput: ~{} examples / second\".format(batch_size * sent_count / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8077f1-f1a8-480b-a58a-75e72609a6e8",
   "metadata": {},
   "source": [
    "The responses we get need to be decoded and converted to a NumPy array. Let's examine the shapes of a sample output after the conversion to NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3dfbd2-13e6-4071-86f4-bfec5f7cc873",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = responses[0]\n",
    "\n",
    "output_names = [\"BatchedNMS\", \"BatchedNMS_1\", \"BatchedNMS_2\", \"BatchedNMS_3\"]\n",
    "output_array = []      \n",
    "  \n",
    "for output_name in output_names:\n",
    "    output_array.append(sample_output.as_numpy(output_name))\n",
    "\n",
    "print([a.shape for a in output_array])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5658cca6-b9fb-406e-8eba-a9a10617545c",
   "metadata": {},
   "source": [
    "We recognize the four output shapes of our model but to convert these numbers into an output we can read and visualize, we pass the responses to a specific postprocessor that renders images with bounding boxes at `$output_path/infer_images` and labels in KITTI format at `$output_path/infer_labels`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cdb7e5-5ec3-4302-b960-c9351f22e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov4_postprocessor import YOLOv4Postprocessor\n",
    "\n",
    "print(\"Gathering responses from the server and post-processing the inferenced outputs \\n\")\n",
    "\n",
    "args_postprocessor = [\n",
    "    batch_size, frames, output_path, triton_model.data_format\n",
    "]\n",
    "\n",
    "postprocessor = YOLOv4Postprocessor(*args_postprocessor)\n",
    "\n",
    "processed_request = 0\n",
    "with tqdm(total=len(frames)) as pbar:\n",
    "    while processed_request < sent_count:\n",
    "        response = responses[processed_request]\n",
    "\n",
    "        this_id = response.get_response()[\"id\"]\n",
    "\n",
    "        postprocessor.apply(\n",
    "            response, this_id, render=True\n",
    "        )\n",
    "        processed_request += 1\n",
    "        pbar.update(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a3d48-a8f0-4412-9a1f-97b161b25440",
   "metadata": {},
   "source": [
    "Let's observe the output on the test images to confirm that the model is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8768a60-7187-4fe4-b3d5-bc62ad67ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid visualizer\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "\n",
    "def visualize_images(image_dir, num_cols=4, num_images=10):\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(image_dir, image) for image in os.listdir(image_dir) \n",
    "         if os.path.splitext(image)[1].lower() == '.png']\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img)\n",
    "        \n",
    "# Visualizing the sample images\n",
    "OUTPUT_PATH = os.path.join(output_path, 'infer_images')\n",
    "COLS = 3 # number of columns in the visualizer grid\n",
    "IMAGES = 9 # number of images to visualize\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0de0a1-7812-40a4-9687-6ae3e9c5bdaa",
   "metadata": {},
   "source": [
    "With this, we have successfully run HTTP inference using Triton with our object detection model and rendered the results in a useful format. As you may have noticed, a lot of work is required for preprocessing and postprocessing of the results, while inference itself does not require a lot of code and could be simplified even more. As inference is at the heart of this lab, we will now look at how to run some performance benchmarks and speed it up even further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727c9f9d-5d32-4e19-8332-654ed5c93b6c",
   "metadata": {},
   "source": [
    "## Performance analyzer\n",
    "\n",
    "While trying to optimize the inference performance of a model, it is critical to be able to measure changes while experimenting with different optimization strategies. Fortunately, the `perf_analyzer` application, included with the client examples which are available from several sources (NGC, GitHub, etc), performs exactly this task for us.\n",
    "\n",
    "The `perf_analyzer` application generates inference requests to a model and measures the throughput and latency of those requests. Latency indicates how long it takes for packets of data to reach their destination. Throughput is the term given to the number of packets that are processed within a specific period of time. To get representative results, `perf_analyzer` measures the throughput and latency over a time window and then repeats the measurements until it gets stable values. By default, `perf_analyzer` uses average latency to determine stability but you can use the `--percentile` flag to stabilize results based on that confidence level. \n",
    "\n",
    "For example, we can run the following command to analyze the performance of our model stabilized using the 95th percentile request latency:\n",
    "\n",
    "```\n",
    "perf_analyzer \\\n",
    "  -m yolov4_tao \\\n",
    "  -b 1 \\\n",
    "  --concurrency-range 1:1 \\\n",
    "  --percentile=95\n",
    "```\n",
    "\n",
    "Here is what the output of a `perf_analyzer` run would look like for our model:\n",
    "\n",
    "```\n",
    "$ perf_analyzer -m yolov4_tao -b 1 --concurrency-range 1:1 --percentile=95\n",
    "*** Measurement Settings ***\n",
    "  Batch size: 1\n",
    "  Using \"time_windows\" mode for stabilization\n",
    "  Measurement window: 5000 msec\n",
    "  Using synchronous calls for inference\n",
    "  Stabilizing using p95 latency\n",
    "\n",
    "Request concurrency: 1\n",
    "  Client: \n",
    "    Request count: 606\n",
    "    Throughput: 121.2 infer/sec\n",
    "    p50 latency: 8194 usec\n",
    "    p90 latency: 8390 usec\n",
    "    p95 latency: 8493 usec\n",
    "    p99 latency: 9058 usec\n",
    "    Avg HTTP time: 8250 usec (send/recv 400 usec + response wait 7850 usec)\n",
    "  Server: \n",
    "    Inference count: 727\n",
    "    Execution count: 727\n",
    "    Successful request count: 727\n",
    "    Avg request latency: 6905 usec (overhead 22 usec + queue 53 usec + compute input 476 usec + compute infer 6316 usec + compute output 37 usec)\n",
    "\n",
    "Inferences/Second vs. Client p95 Batch Latency\n",
    "Concurrency: 1, throughput: 121.2 infer/sec, latency 8493 usec\n",
    "```\n",
    "\n",
    "For more details on `perf_analyzer`, including all the inference strategies supported, call the help function from the command line with `perf_analyzer -h` and have a look at the documentation [here](https://github.com/triton-inference-server/server/blob/r22.07/docs/perf_analyzer.md). Benchmarking Triton has never been so easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27385e1-7609-48ac-99f5-e7bd614aa1e1",
   "metadata": {},
   "source": [
    "## Improve inference performance\n",
    "\n",
    "Now that we know how to measure the impact of different inference choices, here we quickly go through a list of things to help deliver maximum performance. These include variable batch size, dynamic batching, gRPC protocol, and asynchronous inference.\n",
    "\n",
    "### Variable batch size\n",
    "\n",
    "In our example, we have worked with data inputs that have a batch size of 1. However, we might often want to use different batch sizes such as 4, 8, 16, or even higher. This has a natural tradeoff of latency and throughput. Since our batches are larger, it might take longer to process an individual batch - increasing the latency. However, since the GPU has more data to work with and we're less constrained by networking and I/O, we might see an increase in throughput - or the number of examples that can be processed per second. Depending on the application, this might be a good way to go. Feel free to go back and vary the batch size to see the impact it has on latency and throughput.\n",
    "\n",
    "### Dynamic batching\n",
    "\n",
    "For most models, the Triton feature that provides the largest performance improvement is dynamic batching. This is a feature that allows individual inference requests to be combined by the server, creating batches dynamically. As we said just above, creating a batch of requests typically results in increased throughput since it executes much more efficiently on the GPU. To enable dynamic batching, simply add the following:\n",
    "\n",
    "```\n",
    "dynamic_batching { }\n",
    "```\n",
    "\n",
    "to the model configuration file to enable dynamic batching with all default settings. By default, the dynamic batcher will create batches as large as possible up to the maximum batch size and will not delay when forming batches. \n",
    "\n",
    "This behavior can be modified by specifying the `preferred_batch_size property`, which indicates the batch sizes that the dynamic batcher should attempt to create, and the `max_queue_delay_microseconds`, setting the maximum delay in sending an inference request as is (even if not of a preferred size) when a batch of a preferred size cannot be formed. For more information on this, please check the [model configuration](https://github.com/triton-inference-server/server/blob/r22.07/docs/model_configuration.md) and [model optimization](https://github.com/triton-inference-server/server/blob/r22.07/docs/optimization.md) docs.\n",
    "\n",
    "Below, we modify our model configuration file so that Triton Inference Server will deploy it using dynamic batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006fb51c-6f91-431f-9635-1d285558662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 16\n",
    "input [\n",
    "  {\n",
    "    name: \"Input\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 384, 640 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"BatchedNMS\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"BatchedNMS_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 200, 4 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"BatchedNMS_2\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 200 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"BatchedNMS_3\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 200 ]\n",
    "  }\n",
    "]\n",
    "dynamic_batching { }\n",
    "\"\"\"\n",
    "\n",
    "with open(\"../models/yolov4_tao/config.pbtxt\", 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d353019e-eb5e-42d7-a427-24c60ac17f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleep 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b1bfc-00fe-4a78-8236-8bf8932cd7c4",
   "metadata": {},
   "source": [
    "### Asynchronous inference\n",
    "\n",
    "So far, our requests have been submitted to Triton Inference Server synchronously. In other words, we submit a request to Triton, which computes and returns the result, and then we submit our next request. However, it is also possible to submit as many requests as possible, allow Triton to queue requests it hasn't yet processed, and return results as soon as they are computed. This paradigm is known as asynchronous inferencing and can result in some impressive speedups for throughput.\n",
    "\n",
    "### gRPC protocol\n",
    "\n",
    "Last but not least, let's spend a couple of words on switching protocol to gRPC. As we discussed, clients can communicate with Triton using either HTTP or gRPC protocol. Most people are familiar with HTTP, which is the backbone of the internet, but gRPC is a newer, open-source remote procedure call system initially developed at Google in 2015 that uses HTTP/2 for transport and protocol buffers as the interface description language. It is highly efficient and using it is very easy: all you need to do is switch to the `tritonclient.grpc.InferenceServerClient` module, change the inference server URL and make other minimal changes to the pipeline. Using a slightly different protocol can have an enormous impact on latency and throughput, so remember that gRPC exists!\n",
    "\n",
    "\n",
    "## Analyze the impact of inference optimization\n",
    "\n",
    "We will now implement the aforementioned strategies in this notebook and see the effect they have on performance. In particular, we will add both asynchronous inference and gRPC protocol to the pipeline. The model configuration file has already been updated to make use of dynamic batching. Let's import the `tritonclient.grpc` module and set the new url for `gRPC` protocol requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe4e05e-c282-47f3-a0b7-fde9ae4f434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as grpcclient\n",
    "\n",
    "verbose = False\n",
    "url = 'localhost:8001'\n",
    "protocol = 'grpc'\n",
    "batch_size = 1\n",
    "output_path = \"../source_code/N3/triton_output_grpc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4220bc1-b2bb-4a46-bac0-88445432b584",
   "metadata": {},
   "source": [
    "Then, we instantiate the new Triton Client and get access to additional properties from the model metadata and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45eb09c-77a9-40d4-bfc1-e38b2473bf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running the inference client \\n\")\n",
    "\n",
    "try:\n",
    "    # Create gRPC client for communicating with the server\n",
    "    triton_client = grpcclient.InferenceServerClient(\n",
    "        url=url, verbose=verbose)\n",
    "except Exception as e:\n",
    "    print(\"client creation failed: \" + str(e))\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    model_metadata = triton_client.get_model_metadata(\n",
    "        model_name=model_name, model_version=model_version)\n",
    "except InferenceServerException as e:\n",
    "    print(\"failed to retrieve the metadata: \" + str(e))\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    model_config = triton_client.get_model_config(\n",
    "        model_name=model_name, model_version=model_version)\n",
    "except InferenceServerException as e:\n",
    "    print(\"failed to retrieve the config: \" + str(e))\n",
    "    sys.exit(1)\n",
    "\n",
    "model_config = model_config.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6137509f-68f5-4480-83c2-eb15c02ebda0",
   "metadata": {},
   "source": [
    "Images are already loaded so we can go ahead and submit our inputs to the Triton Inference Server using the `triton_client.async_infer()` method, specifying once again our model name, version, inputs and outputs. The responses we get are then stored in an array at the end like before. Below, we also call a utility callback function for handling asynchronous requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a6b00c-0a00-4854-8494-7d51f1289a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from user_data import UserData\n",
    "from functools import partial\n",
    "\n",
    "def completion_callback(user_data, result, error):\n",
    "    \"\"\"Callback function used for async_stream_infer().\"\"\"\n",
    "    user_data._completed_requests.put((result, error))\n",
    "\n",
    "print(\"Sending inference request for batches of data \\n\")\n",
    "\n",
    "responses = []\n",
    "image_idx = 0\n",
    "last_request = False\n",
    "user_data = UserData()\n",
    "sent_count = 0\n",
    "pbar_total = len(frames)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=pbar_total) as pbar:\n",
    "    while not last_request:\n",
    "        batched_image_data = None\n",
    "\n",
    "        repeated_image_data = []\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            frame = frames[image_idx]\n",
    "            \n",
    "            img = frame._load_img()\n",
    "            repeated_image_data.append(img)\n",
    "            \n",
    "            image_idx = (image_idx + 1) % len(frames)\n",
    "            if image_idx == 0:\n",
    "                last_request = True\n",
    "\n",
    "        if max_batch_size > 0:\n",
    "            batched_image_data = np.stack(repeated_image_data, axis=0)\n",
    "        else:\n",
    "            batched_image_data = repeated_image_data[0]\n",
    "\n",
    "        # Send request\n",
    "        try:\n",
    "            req_gen_args = [batched_image_data, triton_model.input_names,\n",
    "                triton_model.output_names, triton_model.triton_dtype,\n",
    "                protocol.lower()]\n",
    "            req_generator = requestGenerator(*req_gen_args)\n",
    "            for inputs, outputs in req_generator:\n",
    "                sent_count += 1\n",
    "\n",
    "                triton_client.async_infer(\n",
    "                    model_name,\n",
    "                    inputs,\n",
    "                    partial(completion_callback, user_data),\n",
    "                    request_id=str(sent_count),\n",
    "                    model_version=model_version,\n",
    "                    outputs=outputs)\n",
    "\n",
    "        except InferenceServerException as e:\n",
    "            print(\"inference failed: \" + str(e))\n",
    "            sys.exit(1)\n",
    "        \n",
    "        pbar.update(batch_size)\n",
    "    \n",
    "    processed_count = 0\n",
    "    while processed_count < sent_count:\n",
    "        (results, error) = user_data._completed_requests.get()\n",
    "        processed_count += 1\n",
    "        if error is not None:\n",
    "            print(\"inference failed: \" + str(error))\n",
    "            sys.exit(1)\n",
    "        responses.append(results)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Average latency: ~{} seconds\".format((end_time - start_time) / sent_count))\n",
    "print(\"Average throughput: ~{} examples / second\".format(batch_size * sent_count / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8e78d-dd39-48e9-8abf-f18dc2f9a1db",
   "metadata": {},
   "source": [
    "As you can see, the gain in performance is quite significant, and considering the very small changes we made to the pipeline, it was definitely worth it!\n",
    "\n",
    "Now we pass the responses to the postprocessor that renders images with bounding boxes and show them to make sure nothing has changed compared to the http inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e0b7d-7833-4eb2-807d-c8d2fe329529",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gathering responses from the server and post-processing the inferenced outputs \\n\")\n",
    "\n",
    "args_postprocessor = [\n",
    "    batch_size, frames, output_path, triton_model.data_format\n",
    "]\n",
    "\n",
    "postprocessor = YOLOv4Postprocessor(*args_postprocessor)\n",
    "\n",
    "processed_request = 0\n",
    "with tqdm(total=len(frames)) as pbar:\n",
    "    while processed_request < sent_count:\n",
    "        response = responses[processed_request]\n",
    "\n",
    "        this_id = response.get_response().id\n",
    "\n",
    "        postprocessor.apply(\n",
    "            response, this_id, render=True\n",
    "        )\n",
    "        processed_request += 1\n",
    "        pbar.update(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3066d09-0577-4511-9b6e-05b34a250ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images\n",
    "OUTPUT_PATH = os.path.join(output_path, 'infer_images')\n",
    "COLS = 3 # number of columns in the visualizer grid\n",
    "IMAGES = 9 # number of images to visualize\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2025698-d439-4bbd-b5c5-41f61dac8fab",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "As we have seen, squeezing every last drop of performance out of a deployed model takes time and a lot of experimentation and profiling, but it can bring great benefits to an application, saving time and money in the long run. \n",
    "\n",
    "If you are interested in improving your Triton knowledge and want to know how to deploy models in other native frameworks like TensorFlow and PyTorch, while also keeping an eye on performance monitoring, you can check the NVIDIA Deep Learning Institute course [Deploying a Model for Inference at Production Scale](https://courses.nvidia.com/courses/course-v1:DLI+S-FX-03+V1) and take a deep dive into the [TAO Toolkit Triton Apps](https://github.com/NVIDIA-AI-IOT/tao-toolkit-triton-apps) GitHub repository.\n",
    "\n",
    "In this notebook, we deployed our model for inference on Triton Inference Server and optimized inference in order to improve performance. Next, we will see an alternative deployment option using DeepStream. \n",
    "\n",
    "Now please go back to the `README` file and follow the instructions there to shut down the server container that is no longer needed and activate the new working environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e115e7f-861f-436c-9ee0-c6dd865a008c",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## References\n",
    "\n",
    "- [1] *https://courses.nvidia.com/courses/course-v1:DLI+S-FX-03+V1*\n",
    "- [2] *https://github.com/NVIDIA-AI-IOT/tao-toolkit-triton-apps*\n",
    "\n",
    "## Licensing\n",
    "\n",
    "Copyright © 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be58d4a7-01b3-4f4b-9489-c8a631839d2d",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"2.Object_detection_using_TAO_YOLOv4.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 34%; text-align: center;\">\n",
    "        <a href=\"1.Data_labeling_and_preprocessing.ipynb\">1</a>\n",
    "        <a href=\"2.Object_detection_using_TAO_YOLOv4.ipynb\">2</a>\n",
    "        <a >3</a>\n",
    "        <a href=\"4.Model_deployment_with_DeepStream.ipynb\">4</a>\n",
    "        <a href=\"5.Measure_object_size_using_OpenCV.ipynb\">5</a>\n",
    "        <a href=\"6.Challenge_DeepStream.ipynb\">6</a>\n",
    "        <a href=\"7.Challenge_Triton.ipynb\">7</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"4.Model_deployment_with_DeepStream.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<p> <center> <a href=\"../Start_here.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
