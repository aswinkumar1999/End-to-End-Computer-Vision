{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../Start_here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"3.Model_deployment_with_Triton_Inference_Server.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 34%; text-align: center;\">\n",
    "        <a href=\"1.Data_labeling_and_preprocessing.ipynb\">1</a>\n",
    "        <a href=\"2.Object_detection_using_TAO_YOLOv4.ipynb\">2</a>\n",
    "        <a href=\"3.Model_deployment_with_Triton_Inference_Server.ipynb\">3</a>\n",
    "        <a >4</a>\n",
    "        <a href=\"5.Measure_object_size_using_OpenCV.ipynb\">5</a>\n",
    "        <a href=\"6.Challenge_DeepStream.ipynb\">6</a>\n",
    "        <a href=\"7.Challenge_Triton.ipynb\">7</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"5.Measure_object_size_using_OpenCV.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model deployment with DeepStream\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of this notebook is to make you understand how to:**\n",
    "\n",
    "- Deploy a TAO Toolkit model to DeepStream using Python bindings\n",
    "- Find objects in a video stream, annotate them with bounding boxes, and output the annotated stream along with a count of the objects found\n",
    "- Potentially scale up your application and build a multi-stream, multi-DNN pipeline\n",
    "\n",
    "**Contents of this notebook:**\n",
    "\n",
    "- [Introduction to DeepStream](#Introduction-to-DeepStream)\n",
    "    - [Seamless integration with TAO Toolkit](#Seamless-integration-with-TAO-Toolkit)\n",
    "    - [Performance and scalability](#Performance-and-scalability)\n",
    "- [Overview of DeepStream SDK](#Overview-of-DeepStream-SDK)\n",
    "- [GStreamer foundations](#GStreamer-foundations)\n",
    "    - [Elements](#Elements)\n",
    "    - [Pipelines](#Pipelines)\n",
    "    - [Pads](#Pads)\n",
    "    - [Caps](#Caps)\n",
    "    - [Buffers](#Buffers)\n",
    "    - [Plugin based architecture](#Plugin-based-architecture)\n",
    "- [Getting started with DeepStream pipeline](#Getting-started-with-DeepStream-pipeline)\n",
    "- [NVIDIA DeepStream plugins](#NVIDIA-DeepStream-plugins)\n",
    "    - [Nvinfer](#Nvinfer)\n",
    "    - [Nvtracker](#Nvtracker)\n",
    "    - [Nvvidconv](#Nvvidconv)\n",
    "    - [Nvosd](#Nvosd)\n",
    "- [Building the pipeline](#Building-the-pipeline)\n",
    "- [Understanding the configuration files](#Understanding-the-configuration-files)\n",
    "- [Working with the metadata](#Working-with-the-metadata)\n",
    "- [Run the pipeline](#Run-the-pipeline)\n",
    "- [Scale up to multi-stream, multi-DNN pipelines](#Scale-up-to-multi-stream,-multi-DNN-pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to DeepStream\n",
    "\n",
    "In this notebook, you will be introduced to DeepStream, its workflow, and the underlying principles on which it is based.\n",
    "\n",
    "![Workflow](images/ds_workflow.png)\n",
    "<div style=\"font-size:11px\">Source: https://developer.nvidia.com/deepstream-sdk</div><br>\n",
    "\n",
    "DeepStream is a powerful streaming analytic toolkit to rapidly develop and deploy AI-powered applications and services. Some popular use cases include: automating industrial processes, robotics, optical inspection, managing logistics, and retail analytics.\n",
    "\n",
    "DeepStream simplifies building Intelligent Video Analytics (IVA) applications by separating them into components managed and built by the user and components efficiently handled behind the scenes for us.\n",
    "\n",
    "As developers, we build components to manage important business tasks like:\n",
    "- Selecting the kind and number of video streams we want to analyze\n",
    "- Choosing the type of analysis we want to do on the video\n",
    "- Handling and interacting with the results of our analysis\n",
    "    \n",
    "We don't need to build components to manage difficult in-between tasks like:\n",
    "- Efficiently leverage the GPU for accelerated processing and inference\n",
    "- Efficiently process data from multiple video streams at once\n",
    "- Keeping track of metadata associated with each frame of video from multiple sources\n",
    "- Optimizing our pipeline for maximum data throughput\n",
    "- Optimizing our neural networks for high-speed inference\n",
    "\n",
    "Having **DeepStream SDK** solving these tasks for us, we can focus more on the valuable parts of the project related to goal and impact. \n",
    "\n",
    "### Seamless integration with TAO Toolkit\n",
    "\n",
    "DeepStream offers turnkey integration of models trained with **TAO Toolkit**, speeding up overall development and deployment efforts and unlocking greater real-time performance by building an end-to-end vision AI pipeline together. With native integration to NVIDIA [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server), we can also deploy models in native frameworks such as **PyTorch** and **TensorFlow** for inference. \n",
    "\n",
    "In this notebook, we will pick up exactly where we left off in the previous one and visualize our TAO model at work on a video stream.\n",
    "\n",
    "<img src=\"images/tao_deepstream.jpeg\" width=\"720\">\n",
    "<div style=\"width:720px; font-size:11px\">Source: https://docs.nvidia.com/tao/tao-toolkit</div><br>\n",
    "\n",
    "### Performance and scalability\n",
    "\n",
    "DeepStream offers exceptional throughput for a wide variety of object detection, image classification and instance segmentation based AI models. The DeepStream SDK allows streaming video from multiple sources and simultaneous optimization of video decode/encode, image scaling and conversion and edge-to-cloud connectivity for complete end-to-end performance optimization.\n",
    "\n",
    "DeepStream provides also scalability at different levels of the system hierarchy. For example: \n",
    "- DeepStream SDK 3.0 supports processing a higher number of concurrent streams, in addition to utilizing multiple GPUs upon availability\n",
    "- DeepStream SDK 4.0 delivers a unified code base for all NVIDIA GPUs and quick integration with IoT services\n",
    "- DeepStream SDK 5.0 adds support for NVIDIA Triton Inference Server, Python bindings, and Instance segmentation with Mask R-CNN\n",
    "- DeepStream SDK 6.0 integrates NVIDIA TAO Toolkit models and introduces Graph Composer, new sample applications, and new plugins\n",
    "\n",
    "Furthermore, DeepStream containers provide flexibility to the deployment phase and now come with libraries like CUDA, cuDNN and TensorRT already installed inside them along with the source code for reference applications. In a typical scenario, you build, execute and debug a DeepStream application within the DeepStream container. These containers are available at `ngc.nvidia.com`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of DeepStream SDK\n",
    "\n",
    "DeepStream SDK consists of a set of building blocks that bridge the gap between low-level APIs (such as TensorRT, Video Codec SDK) and the user application that takes streaming data input from USB/CSI camera, video from file or streams over RTSP, and uses AI and computer vision to generate insights from pixels for better understanding of the environment. DeepStream supports application development in C/C++ and in Python through the Python bindings.\n",
    "DeepStream builds on top of several NVIDIA libraries from the CUDA-X stack such as `CUDA`, `TensorRT`, `NVIDIA® Triton™ Inference server` and `multimedia libraries`. DeepStream abstracts these libraries in DeepStream plugins, making it easy for developers to build video analytic pipelines without having to learn all the individual libraries. For more details visit [here](https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Overview.html).\n",
    "\n",
    "<img src=\"images/deepstream_overview.jpg\" width=\"720\">\n",
    "<div style=\"font-size:11px\">Source: https://developer.nvidia.com/deepstream-sdk</div><br>\n",
    "\n",
    "The DeepStream Python application uses the `Gst-Python` API action to construct the pipeline and use probe functions to access data at various points in the pipeline. [Click here](https://github.com/NVIDIA-AI-IOT/deepstream_python_apps/tree/master/bindings) to see the steps and prerequisites to build the custom Python binding wheel.\n",
    "\n",
    "<img src=\"images/deepstream_python_bindings.png\" width=\"720\">\n",
    "<div style=\"font-size:11px\">Source: https://docs.nvidia.com/metropolis/deepstream/dev-guide</div><br>\n",
    "\n",
    "Below, you can see a short list of the new capabilities provided by DeepStream:\n",
    "- Allowing addition and removal of video stream dynamically and during the pipeline execution, in addition to frame rate and resolution adjustments\n",
    "- Extending the video processing capabilities by supporting custom layers, and user-defined parsing of detector outputs\n",
    "- Providing Support for a 360-degree camera using GPU-accelerated dewarping libraries\n",
    "- Augmenting the meta-data with application-specific, user-defined insights\n",
    "- Providing pruned and efficient inference models\n",
    "- Getting detailed performance analysis with the NVIDIA Nsight Systems profiler tool\n",
    "\n",
    "DeepStream SDK is based on the **GStreamer multimedia framework** and provides a pipeline of GPU accelerated plugins as shown below. \n",
    "\n",
    "The SDK facilitates the application implementation procedure by providing plugins for video inputs, video decoding, image preprocessing, TensorRT-based inference, object tracking and display, and many more. You can use these capabilities to assemble flexible, multi-GPU, multi-stream video analytics applications.\n",
    "\n",
    "<img src=\"images/ds_overview.png\" width=\"720\">\n",
    "<div style=\"width:720px; font-size:11px\">Source: https://docs.nvidia.com/metropolis/deepstream/dev-guide</div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GStreamer foundations\n",
    "\n",
    "The DeepStream SDK is based on the open source [GStreamer multimedia framework](https://gstreamer.freedesktop.org/). There are a few key concepts in GStreamer that we need to touch on before getting started. These include Elements, Pipelines, Pads, Caps, and Buffers. We will be describing them at a high level, but encourage those who are interested in the details to read the [GStreamer Basics](https://gstreamer.freedesktop.org/documentation/?gi-language=python) to learn more.\n",
    "\n",
    "### Elements \n",
    "\n",
    "Elements are the core building block with which we make pipelines. Every process in-between the source (i.e. input of the pipeline, e.g. camera and video files) and sink elements (e.g. screen display) is passed through elements. Video decoding and encoding, neural network inference, and displaying text on top of video streams are examples of an element. DeepStream allows us to instantiate elements and weave them into pipelines.\n",
    "\n",
    "### Pipelines \n",
    "\n",
    "All elements in GStreamer must typically be contained inside a pipeline before they can be used because it takes care of some clocking and messaging functions. A pipeline is a particular type of bin, which is the element used to contain other elements. Therefore all methods which apply to bins also apply to pipelines. We need to add the elements to the pipeline and then link them. This linking must be established following the data flow (this is, from source elements to sink elements).\n",
    "\n",
    "![pipeline](images/pipeline.png)\n",
    "\n",
    "### Pads\n",
    "\n",
    "Pads are the interfaces between elements. When data flows from one element to another element in a pipeline, it flows from the sink pad of one element to the source pad of another. Note that each element might have zero, one, or many source/sink elements.\n",
    "\n",
    "![pads](images/pads.png)\n",
    "\n",
    "### Caps\n",
    "\n",
    "Caps (or Capabilities) are the data types that a pad is permitted to utilize or emit. Because pads can allow multiple data types, sometimes the data flow is ambiguous. Pads are negotiated in order to explicitly define the type of data that can flow through the pad. Caps streamline this process and allow elements of our pipeline with ambiguous pads to negotiate the correct data flow process. Later in this course, we will use caps to pass certain video data types (NV12, RGB) to the downstream elements in the pipeline.\n",
    "\n",
    "### Buffers\n",
    "\n",
    "Buffers carry the data that will pass through the pipeline. Buffers are timestamped and contain metadata such as how many elements are using it, flags, and pointers to objects in memory. When we write application code, we rely on accessing data attached to the buffer.\n",
    "\n",
    "### Plugin based architecture\n",
    "\n",
    "DeepStream applications can be thought of as pipelines consisting of individual components (plugins). Each plugin represents a functional block like inference using TensorRT or multi-stream decode. Where applicable, plugins are accelerated using the underlying hardware to deliver maximum performance. DeepStream’s key value is in making deep learning for video easily accessible, to allow you to concentrate on quickly building and customizing efficient and scalable video analytics applications.\n",
    "\n",
    "The plugin architecture provides functionality such as video encoding/decoding, scaling, inferencing, and more. By connecting plugins into a pipeline, we can build complex applications. Because DeepStream is built on top of GStreamer, we can inspect plugins using `gst-inspect-1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the nvinfer plugin\n",
    "!gst-inspect-1.0 nvinfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Getting started with DeepStream pipeline\n",
    "\n",
    "In this notebook, you will get started with DeepStream Python bindings and get an idea of the workflow to build a 6-class object detection pipeline with a tracker assigning unique IDs to objects. The 6-class object detection pipeline has a structure similar to the one shown in the illustration below, but the second stage classifiers (highlighted in gray) are not implemented in this case.\n",
    "\n",
    "<img src=\"images/test2.png\" width=\"1080\">\n",
    "\n",
    "We notice there are multiple DeepStream plugins used in the pipeline, let's have a look and try to understand them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDIA DeepStream plugins\n",
    "\n",
    "### Nvinfer\n",
    "\n",
    "The nvinfer plugin provides TensorRT-based inference for detection and tracking. The low-level library (libnvds_infer) operates on any of RGB, BGR, or GRAY data with dimension of Network Height and Network Width. The plugin accepts NV12/RGBA data from upstream components like the decoder, muxer, and dewarper.\n",
    "The Gst-nvinfer plugin also performs preprocessing operations like format conversion, scaling, mean subtraction, and produces final float RGB/BGR/GRAY planar data which is passed to the low-level library. The low-level library uses the TensorRT engine for inferencing. It outputs each classified object’s class and each detected object’s bounding boxes (Bboxes) after clustering.\n",
    "\n",
    "<img src=\"images/nvinfer.png\" width=\"720\">\n",
    "<div style=\"font-size:11px\">Source: https://docs.nvidia.com/metropolis/deepstream/dev-guide</div><br>\n",
    "\n",
    "### Nvtracker\n",
    "\n",
    "This plugin allows the DS pipeline to use a low-level tracker library to track the detected objects with persistent (possibly unique) IDs over time. The plugin accepts NV12/RGBA data from the upstream component and scales (and/or converts) the input buffer to a buffer with a specific tracker width and height (`tracker-width` and `tracker-height` must be specified in the configuration file’s `[tracker]` section).\n",
    "\n",
    "The path to the low-level tracker library is to be specified via the `ll-lib-file` configuration option in the same section. The low-level library to be used may also require its own configuration file, which can be specified via the `ll-config-file` option. The reference low-level tracker implementations support different tracking algorithms, including the Intersection-Over-Union (IOU) tracker algorithm to determine the object's unique ID, which uses the intersection of the detector’s bounding boxes between two consecutive frames to perform the association between them or assign a new target ID if no match found.\n",
    "\n",
    "DeepStream 6.0 introduced a unified low-level tracker library named `libnvds_nvmultiobjecttracker.so`, therefore the tracker configuration file depending on your tracker choice will be the same for every algorithm.\n",
    "\n",
    "<img src=\"images/nvtracker.png\" width=\"720\">\n",
    "<div style=\"font-size:11px\">Source: https://docs.nvidia.com/metropolis/deepstream/dev-guide</div><br>\n",
    "\n",
    "### Nvvidconv \n",
    "\n",
    "The nvvidconv plugin performs video color format conversion, which is required to make data ready for the nvosd plugin.\n",
    "\n",
    "<img src=\"images/nvvidconv.png\" width=\"720\">\n",
    "<div style=\"font-size:11px\">Source: https://docs.nvidia.com/metropolis/deepstream/dev-guide</div><br>\n",
    "\n",
    "### Nvosd\n",
    "\n",
    "The nvosd plugin draws bounding boxes, text, and region of interest (RoI) polygons (polygons are presented as a set of lines). The plugin accepts an RGBA buffer with attached metadata from the upstream component. It draws bounding boxes, which may be shaded depending on the configuration (e.g. width, color, and opacity) of a given bounding box. It also draws text and RoI polygons at specified locations in the frame. Text and polygon parameters are configurable through metadata.\n",
    "\n",
    "<img src=\"images/nvosd.png\" width=\"720\">\n",
    "<div style=\"font-size:11px\">Source: https://docs.nvidia.com/metropolis/deepstream/dev-guide</div><br>\n",
    "\n",
    "Now with that in mind, let's get started building the pipeline.\n",
    "\n",
    "## Building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append(\"../source_code/N4\")\n",
    "from bus_call import bus_call\n",
    "import gi\n",
    "gi.require_version('Gst', '1.0')\n",
    "from gi.repository import GLib, Gst\n",
    "import configparser\n",
    "import pyds\n",
    "import time\n",
    "\n",
    "# Define class labels\n",
    "PGIE_CLASS_ID_FRESHAPPLE = 0\n",
    "PGIE_CLASS_ID_FRESHBANANA = 1\n",
    "PGIE_CLASS_ID_FRESHORANGE = 2\n",
    "PGIE_CLASS_ID_ROTTENAPPLE = 3\n",
    "PGIE_CLASS_ID_ROTTENBANANA = 4\n",
    "PGIE_CLASS_ID_ROTTENORANGE = 5\n",
    "\n",
    "# Define input/output video files\n",
    "INPUT_VIDEO_NAME  = \"../source_code/N4/apples.h264\" # Source: https://depositphotos.com\n",
    "OUTPUT_VIDEO_NAME = \"../source_code/N4/ds_out.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a function `make_elm_or_print_err()` to create our elements and report any errors if the creation fails. Elements are created using the `Gst.ElementFactory.make()` function as part of Gstreamer library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make element or print error and any other detail\n",
    "def make_elm_or_print_err(factoryname, name, printedname, detail=\"\"):\n",
    "    print(\"Creating\", printedname)\n",
    "    elm = Gst.ElementFactory.make(factoryname, name)\n",
    "    if not elm:\n",
    "        sys.stderr.write(\"Unable to create \" + printedname + \" \\n\")\n",
    "    if detail:\n",
    "        sys.stderr.write(detail)\n",
    "    return elm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize GStreamer and create an empty pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard GStreamer initialization\n",
    "Gst.init(None)\n",
    "\n",
    "# Create gstreamer elements\n",
    "# Create Pipeline element that will form a connection of other elements\n",
    "print(\"Creating Pipeline \\n\")\n",
    "pipeline = Gst.Pipeline()\n",
    "\n",
    "if not pipeline:\n",
    "    sys.stderr.write(\" Unable to create Pipeline \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create the elements that are required for our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Create elements required for the Pipeline ###########\n",
    "# Source element for reading from the file\n",
    "source = make_elm_or_print_err(\"filesrc\", \"file-source\", \"Source\")\n",
    "# Since the data format in the input file is elementary h264 stream, we need a h264parser\n",
    "h264parser = make_elm_or_print_err(\"h264parse\", \"h264-parser\", \"h264 parse\")\n",
    "# Use nvdec_h264 for hardware accelerated decode on GPU\n",
    "decoder = make_elm_or_print_err(\"nvv4l2decoder\", \"nvv4l2-decoder\", \"Nvv4l2 Decoder\")\n",
    "# Create nvstreammux instance to form batches from one or more sources\n",
    "streammux = make_elm_or_print_err(\"nvstreammux\", \"Stream-muxer\", \"NvStreamMux\")\n",
    "# Use nvinfer to run inferencing on decoder's output, behavior of inferencing is set through config file\n",
    "pgie = make_elm_or_print_err(\"nvinfer\", \"primary-inference\", \"pgie\")\n",
    "# Use nvtracker to give objects unique-ids\n",
    "tracker = make_elm_or_print_err(\"nvtracker\", \"tracker\", \"tracker\")\n",
    "# Use convertor to convert from NV12 to RGBA as required by nvosd\n",
    "nvvidconv = make_elm_or_print_err(\"nvvideoconvert\", \"convertor\", \"nvvidconv\")\n",
    "# Create OSD to draw on the converted RGBA buffer\n",
    "nvosd = make_elm_or_print_err(\"nvdsosd\", \"onscreendisplay\", \"nvosd\")\n",
    "# Finally encode and save the osd output\n",
    "queue = make_elm_or_print_err(\"queue\", \"queue\", \"Queue\")\n",
    "# Use convertor to convert from NV12 to RGBA as required by nvosd\n",
    "nvvidconv2 = make_elm_or_print_err(\"nvvideoconvert\", \"convertor2\", \"nvvidconv2\")\n",
    "# Place an encoder instead of OSD to save as video file\n",
    "encoder = make_elm_or_print_err(\"avenc_mpeg4\", \"encoder\", \"Encoder\")\n",
    "# Parse output from Encoder\n",
    "codeparser = make_elm_or_print_err(\"mpeg4videoparse\", \"mpeg4-parser\", \"Code Parser\")\n",
    "# Create a container\n",
    "container = make_elm_or_print_err(\"qtmux\", \"qtmux\", \"Container\")\n",
    "# Create Sink for storing the output\n",
    "sink = make_elm_or_print_err(\"filesink\", \"filesink\", \"Sink\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the elements, we can proceed to set various properties for our pipeline.\n",
    "\n",
    "## Understanding the configuration files\n",
    "\n",
    "We set a `config-file-path` for our nvinfer (interference plugin) pointing to the file `pgie_yolov4_tao_config.txt`. In this configuration file, the `[property]` group configures the general behavior of the plugin and it is the only mandatory group. Additionally, the `[class-attrs-all]` group configures detection parameters for all classes while the `[class-attrs-<class-id>]` group does the same job but for a particular class specified by `<class-id>`. A list of the keys supported for `[property]` and `[class-attrs-…]` groups is available [here](https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvinfer.html#gst-nvinfer-file-configuration-specifications).\n",
    "    \n",
    "You can take a look at the `pgie_yolov4_tao_config.txt` configuration file below and identify these parts. In this example, we use model-file `resnet18` and let nvinfer create a TensorRT engine specific to the host GPU to accelerate its inference performance.\n",
    "\n",
    "Please make sure to set your API key in the configuration file [here](../source_code/N4/pgie_yolov4_tao_config.txt) in the field `tlt-model-key`, then save the file with `ctrl s`. Not doing so makes it impossible to decrypt the model and successfully run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Please replace the tlt-model-key variable with your key.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../source_code/N4/pgie_yolov4_tao_config.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the configuration file for our nvtracker (tracking plugin) named `dstest2_tracker_config.txt`. The configuration file is parsed and properties are then set for the tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../source_code/N4/dstest2_tracker_config.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we set the properties for the elements of our pipeline, including but not limited to the contents of the two configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Set properties for the Elements ############\n",
    "print(\"Playing file \", INPUT_VIDEO_NAME)\n",
    "# Set Input File Name \n",
    "source.set_property(\"location\", INPUT_VIDEO_NAME)\n",
    "# Set Input Width, Height and Batch Size \n",
    "streammux.set_property(\"width\", 1920)\n",
    "streammux.set_property(\"height\", 1080)\n",
    "streammux.set_property(\"batch-size\", 1)\n",
    "# Timeout in microseconds to wait after the first buffer is available \n",
    "# to push the batch even if a complete batch is not formed.\n",
    "streammux.set_property(\"batched-push-timeout\", 4000000)\n",
    "# Set Congifuration file for nvinfer \n",
    "pgie.set_property(\"config-file-path\", \"../source_code/N4/pgie_yolov4_tao_config.txt\")\n",
    "#Set properties of tracker from tracker_config\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../source_code/N4/dstest2_tracker_config.txt\")\n",
    "config.sections()\n",
    "for key in config['tracker']:\n",
    "    if key == 'tracker-width' :\n",
    "        tracker_width = config.getint('tracker', key)\n",
    "        tracker.set_property('tracker-width', tracker_width)\n",
    "    if key == 'tracker-height' :\n",
    "        tracker_height = config.getint('tracker', key)\n",
    "        tracker.set_property('tracker-height', tracker_height)\n",
    "    if key == 'gpu-id' :\n",
    "        tracker_gpu_id = config.getint('tracker', key)\n",
    "        tracker.set_property('gpu_id', tracker_gpu_id)\n",
    "    if key == 'll-lib-file' :\n",
    "        tracker_ll_lib_file = config.get('tracker', key)\n",
    "        tracker.set_property('ll-lib-file', tracker_ll_lib_file)\n",
    "    if key == 'll-config-file' :\n",
    "        tracker_ll_config_file = config.get('tracker', key)\n",
    "        tracker.set_property('ll-config-file', tracker_ll_config_file)\n",
    "    if key == 'enable-batch-process' :\n",
    "        tracker_enable_batch_process = config.getint('tracker', key)\n",
    "        tracker.set_property('enable_batch_process', tracker_enable_batch_process)\n",
    "# Set Encoder bitrate for output video\n",
    "encoder.set_property(\"bitrate\", 2000000)\n",
    "# Set Output file name and disable sync and async\n",
    "sink.set_property(\"location\", OUTPUT_VIDEO_NAME)\n",
    "sink.set_property(\"sync\", 0)\n",
    "sink.set_property(\"async\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now link all the elements in the order we prefer and create Gstreamer bus to feed all messages through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Add and Link Elements in the Pipeline ##########\n",
    "\n",
    "print(\"Adding elements to Pipeline \\n\")\n",
    "\n",
    "pipeline.add(source)\n",
    "pipeline.add(h264parser)\n",
    "pipeline.add(decoder)\n",
    "pipeline.add(streammux)\n",
    "pipeline.add(pgie)\n",
    "pipeline.add(tracker)\n",
    "pipeline.add(nvvidconv)\n",
    "pipeline.add(nvosd)\n",
    "pipeline.add(queue)\n",
    "pipeline.add(nvvidconv2)\n",
    "pipeline.add(encoder)\n",
    "pipeline.add(codeparser)\n",
    "pipeline.add(container)\n",
    "pipeline.add(sink)\n",
    "\n",
    "# We now  link the elements together \n",
    "# file-source -> h264-parser -> nvh264-decoder -> nvinfer -> nvvidconv ->\n",
    "# queue -> nvvidconv2 -> encoder -> parser -> container -> sink -> output-file\n",
    "print(\"Linking elements in the Pipeline \\n\")\n",
    "source.link(h264parser)\n",
    "h264parser.link(decoder)\n",
    "\n",
    "##### Creating Sink pad and source pads and linking them together \n",
    "\n",
    "# Create Sinkpad to Streammux \n",
    "sinkpad = streammux.get_request_pad(\"sink_0\")\n",
    "if not sinkpad:\n",
    "    sys.stderr.write(\" Unable to get the sink pad of streammux \\n\")\n",
    "# Create source pad from Decoder   \n",
    "srcpad = decoder.get_static_pad(\"src\")\n",
    "if not srcpad:\n",
    "    sys.stderr.write(\" Unable to get source pad of decoder \\n\")\n",
    "    \n",
    "srcpad.link(sinkpad)\n",
    "streammux.link(pgie)\n",
    "pgie.link(tracker)\n",
    "tracker.link(nvvidconv)\n",
    "nvvidconv.link(nvosd)\n",
    "nvosd.link(queue)\n",
    "queue.link(nvvidconv2)\n",
    "nvvidconv2.link(encoder)\n",
    "encoder.link(codeparser)\n",
    "codeparser.link(container)\n",
    "container.link(sink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create an event loop and feed GStreamer bus messages to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = GLib.MainLoop()\n",
    "bus = pipeline.get_bus()\n",
    "bus.add_signal_watch()\n",
    "bus.connect (\"message\", bus_call, loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the metadata\n",
    "\n",
    "Our pipeline now carries the metadata forward but does nothing with it up to this moment. As mentioned in the above pipeline diagram, we will now create a callback function to display relevant data on the frame once it is called and create a sink pad in the `nvosd` element to call the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Working with the Metadata ################\n",
    "\n",
    "def osd_sink_pad_buffer_probe(pad, info, u_data):\n",
    "    \n",
    "    # Intiallizing object counter with 0\n",
    "    obj_counter = {\n",
    "        PGIE_CLASS_ID_FRESHAPPLE:0,\n",
    "        PGIE_CLASS_ID_FRESHBANANA:0,\n",
    "        PGIE_CLASS_ID_FRESHORANGE:0,\n",
    "        PGIE_CLASS_ID_ROTTENAPPLE:0,\n",
    "        PGIE_CLASS_ID_ROTTENBANANA:0,\n",
    "        PGIE_CLASS_ID_ROTTENORANGE:0\n",
    "    }\n",
    "    # Colors of the bounding boxes in RGBA\n",
    "    obj_colors = {\n",
    "        PGIE_CLASS_ID_FRESHAPPLE:(1.0, 0.0, 0.0, 0.0),\n",
    "        PGIE_CLASS_ID_FRESHBANANA:(0.0, 1.0, 0.0, 0.0),\n",
    "        PGIE_CLASS_ID_FRESHORANGE:(0.0, 0.0, 1.0, 0.0),\n",
    "        PGIE_CLASS_ID_ROTTENAPPLE:(0.0, 1.0, 1.0, 0.0),\n",
    "        PGIE_CLASS_ID_ROTTENBANANA:(1.0, 0.0, 1.0, 0.0),\n",
    "        PGIE_CLASS_ID_ROTTENORANGE:(1.0, 1.0, 0.0, 0.0)\n",
    "    }\n",
    "    # Set frame_number & rectangles to draw as 0 \n",
    "    frame_number=0\n",
    "    num_rects=0\n",
    "    \n",
    "    gst_buffer = info.get_buffer()\n",
    "    if not gst_buffer:\n",
    "        print(\"Unable to get GstBuffer \")\n",
    "        return\n",
    "\n",
    "    # Retrieve batch metadata from the gst_buffer\n",
    "    # Note that pyds.gst_buffer_get_nvds_batch_meta() expects the\n",
    "    # C address of gst_buffer as input, which is obtained with hash(gst_buffer)\n",
    "    batch_meta = pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer))\n",
    "    l_frame = batch_meta.frame_meta_list\n",
    "    \n",
    "    while l_frame is not None:\n",
    "        try:\n",
    "            # Note that l_frame.data needs a cast to pyds.NvDsFrameMeta\n",
    "            frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "        # Get frame number, number of rectangles to draw and object metadata\n",
    "        frame_number=frame_meta.frame_num\n",
    "        num_rects = frame_meta.num_obj_meta\n",
    "        l_obj=frame_meta.obj_meta_list\n",
    "        \n",
    "        while l_obj is not None:\n",
    "            try:\n",
    "                # Casting l_obj.data to pyds.NvDsObjectMeta\n",
    "                obj_meta=pyds.NvDsObjectMeta.cast(l_obj.data)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            # Increment object class by 1 and set box border color  \n",
    "            obj_counter[obj_meta.class_id] += 1\n",
    "            r, g, b, a = obj_colors[obj_meta.class_id]\n",
    "            obj_meta.rect_params.border_color.set(r, g, b, a)\n",
    "            try: \n",
    "                l_obj=l_obj.next\n",
    "            except StopIteration:\n",
    "                break\n",
    "        ################## Setting Metadata Display configruation ############### \n",
    "        # Acquiring a display meta object\n",
    "        display_meta=pyds.nvds_acquire_display_meta_from_pool(batch_meta)\n",
    "        display_meta.num_labels = 1\n",
    "        py_nvosd_text_params = display_meta.text_params[0]\n",
    "        # Setting display text to be shown on screen\n",
    "        py_nvosd_text_params.display_text = \"Frame Number={} Number of Objects={} Freshapple_count={} Freshbanana_count={} \" \\\n",
    "            \"Freshorange_count={} Rottenapple_count={} Rottenbanana_count={} Rottenorange_count={}\".format(frame_number, num_rects, \n",
    "            obj_counter[PGIE_CLASS_ID_FRESHAPPLE], obj_counter[PGIE_CLASS_ID_FRESHBANANA], obj_counter[PGIE_CLASS_ID_FRESHORANGE], \n",
    "            obj_counter[PGIE_CLASS_ID_ROTTENAPPLE], obj_counter[PGIE_CLASS_ID_ROTTENBANANA], obj_counter[PGIE_CLASS_ID_ROTTENORANGE])\n",
    "        \n",
    "        # Now set the offsets where the string should appear\n",
    "        py_nvosd_text_params.x_offset = 10\n",
    "        py_nvosd_text_params.y_offset = 12\n",
    "        # Font, font-color and font-size\n",
    "        py_nvosd_text_params.font_params.font_name = \"Serif\"\n",
    "        py_nvosd_text_params.font_params.font_size = 14\n",
    "        # Set(red, green, blue, alpha); Set to White\n",
    "        py_nvosd_text_params.font_params.font_color.set(1.0, 1.0, 1.0, 1.0)\n",
    "        # Text background color\n",
    "        py_nvosd_text_params.set_bg_clr = 1\n",
    "        # Set(red, green, blue, alpha); set to Black\n",
    "        py_nvosd_text_params.text_bg_clr.set(0.0, 0.0, 0.0, 1.0)\n",
    "        # Using pyds.get_string() to get display_text as string to print in notebook\n",
    "        print(pyds.get_string(py_nvosd_text_params.display_text))\n",
    "        pyds.nvds_add_display_meta_to_frame(frame_meta, display_meta)\n",
    "        \n",
    "        ############################################################################\n",
    "        \n",
    "        try:\n",
    "            l_frame=l_frame.next\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return Gst.PadProbeReturn.OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we add the probe to get informed of the meta data generated. We add probe to the sink pad of the osd element, since by that time, the buffer would have got all the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osdsinkpad = nvosd.get_static_pad(\"sink\")\n",
    "if not osdsinkpad:\n",
    "    sys.stderr.write(\" Unable to get sink pad of nvosd \\n\")\n",
    "    \n",
    "osdsinkpad.add_probe(Gst.PadProbeType.BUFFER, osd_sink_pad_buffer_probe, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline\n",
    "\n",
    "Now with everything defined, we can start the playback and listen to the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start play back and listen to events\n",
    "print(\"Starting pipeline \\n\")\n",
    "start_time = time.time()\n",
    "pipeline.set_state(Gst.State.PLAYING)\n",
    "try:\n",
    "    loop.run()\n",
    "except:\n",
    "    pass\n",
    "# cleanup\n",
    "pipeline.set_state(Gst.State.NULL)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the next cell, we convert the video profile to be compatible with Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -loglevel panic -y -an -i ../source_code/N4/ds_out.mp4 -vcodec libx264 -pix_fmt yuv420p -profile:v baseline -level 3 ../source_code/N4/output.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the output\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    " <video width=\"640\" height=\"480\" controls>\n",
    " <source src=\"../source_code/N4/output.mp4\"\n",
    " </video>\n",
    "\"\"\".format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale up to multi-stream, multi-DNN pipelines\n",
    "\n",
    "In this notebook, we learned how to build a pipeline to run inference on a single stream with an object detection model trained with TAO Toolkit. However, DeepStream also provides tools for multi-stream and multi-stage pipeline implementation. For example, we might be interested in capturing video footage from multiple cameras simultaneously and setting up an object detection, tracking, and attribute classification pipeline.\n",
    "\n",
    "If you are interested in building more complex pipelines like this, we encourage you to check the [DeepStream Python Apps](https://github.com/NVIDIA-AI-IOT/deepstream_python_apps) GitHub repository and go through the [DeepStream Bootcamp](https://github.com/openhackathons-org/gpubootcamp/tree/master/ai/DeepStream). If your focus is more on understanding the performance optimization cycle using profilers, then this other Bootcamp on [DeepStream Pipeline Optimization using Profiling](https://github.com/gpuhackathons-org/gpubootcamp/tree/master/ai/DeepStream_Perf_Lab) might be of your interest as well.\n",
    "\n",
    "In this notebook, we have seen how to deploy our TAO model to DeepStream for Intelligent Video Analytics (IVA) applications. Next, we will see how to use OpenCV to estimate the size of objects and how to incorporate this into our machine learning pipeline to gather additional information. Please go to the next notebook by clicking on the `Next Notebook` button below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## References\n",
    "\n",
    "- [1] *https://github.com/NVIDIA-AI-IOT/deepstream_python_apps*\n",
    "- [2] *https://github.com/NVIDIA-AI-IOT/deepstream_tao_apps*\n",
    "\n",
    "## Licensing\n",
    "\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"3.Model_deployment_with_Triton_Inference_Server.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 34%; text-align: center;\">\n",
    "        <a href=\"1.Data_labeling_and_preprocessing.ipynb\">1</a>\n",
    "        <a href=\"2.Object_detection_using_TAO_YOLOv4.ipynb\">2</a>\n",
    "        <a href=\"3.Model_deployment_with_Triton_Inference_Server.ipynb\">3</a>\n",
    "        <a >4</a>\n",
    "        <a href=\"5.Measure_object_size_using_OpenCV.ipynb\">5</a>\n",
    "        <a href=\"6.Challenge_DeepStream.ipynb\">6</a>\n",
    "        <a href=\"7.Challenge_Triton.ipynb\">7</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"5.Measure_object_size_using_OpenCV.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<p> <center> <a href=\"../Start_here.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
