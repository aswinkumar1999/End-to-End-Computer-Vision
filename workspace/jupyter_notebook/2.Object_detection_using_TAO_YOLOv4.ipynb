{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../Start_here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"1.Data_labeling_and_preprocessing.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 34%; text-align: center;\">\n",
    "        <a href=\"1.Data_labeling_and_preprocessing.ipynb\">1</a>\n",
    "        <a >2</a>\n",
    "        <a href=\"3.Model_deployment_with_Triton_Inference_Server.ipynb\">3</a>\n",
    "        <a href=\"4.Model_deployment_with_DeepStream.ipynb\">4</a>\n",
    "        <a href=\"5.Measure_object_size_using_OpenCV.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"3.Model_deployment_with_Triton_Inference_Server.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection using TAO YOLOv4\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of this notebook is to make you understand how to:**\n",
    "\n",
    "- Perform offline data augmentation to increase the dataset size\n",
    "- Take a pretrained resnet18 model and train a ResNet-18 Yolo_v4 model on a dataset in KITTI format\n",
    "- Prune the trained Yolo_v4 model\n",
    "- Retrain the pruned model to recover lost accuracy\n",
    "- Quantize the pruned model using QAT\n",
    "- Export the pruned model\n",
    "- Run inference on the trained model\n",
    "- Export the pruned, quantized and retrained model to a .etlt file for deployment to DeepStream\n",
    "- Run inference on the exported .etlt model to verify deployment using TensorRT\n",
    "\n",
    "**Contents of this notebook:**\n",
    "\n",
    "- [Transfer learning with TAO](#Transfer-learning-with-TAO)\n",
    "    - [API key](#API-key)\n",
    "- [Set up env variables and map drives](#Set-up-env-variables-and-map-drives)\n",
    "- [Offline data augmentation](#Offline-data-augmentation)\n",
    "    - [Configuring the augmentor](#Configuring-the-augmentor)\n",
    "    - [Generate the augmentation spec file](#Generate-the-augmentation-spec-file)\n",
    "    - [Augment the dataset](#Augment-the-dataset)\n",
    "    - [Visualize augmented results](#Visualize-augmented-results)\n",
    "- [Prepare dataset and pre-trained model](#Prepare-dataset-and-pre-trained-model)\n",
    "    - [Generate anchor shape](#Generate-anchor-shape)\n",
    "    - [Generate TFRecords](#Generate-TFRecords)\n",
    "    - [Download pre-trained model](#Download-pre-trained-model)\n",
    "- [Provide training specification](#Provide-training-specification)\n",
    "- [Run TAO training](#Run-TAO-training)\n",
    "- [Evaluate trained model](#Evaluate-trained-model)\n",
    "- [Prune trained model](#Prune-trained-model)\n",
    "- [Retrain pruned model](#Retrain-pruned-model)\n",
    "- [Evaluate retrained model](#Evaluate-retrained-model)\n",
    "- [Visualize inferences](#Visualize-inferences)\n",
    "- [Model export](#Model-export)\n",
    "- [Generate TensorRT engine](#Generate-TensorRT-engine)\n",
    "- [Verify deployed model](#Verify-deployed-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning with TAO\n",
    "\n",
    "In our previous notebook, we saw how to annotate a new dataset for object detection and how to convert it to KITTI format in order to perform transfer learning within the TAO Toolkit. Transfer learning is the commonly used process of transferring learned features from one domain to another with minimal effort by taking a model trained on one task and re-training it to perform a different task. \n",
    "\n",
    "Train Adapt Optimize (TAO) Toolkit by NVIDIA is a simple and easy-to-use Python-based AI toolkit for taking purpose-built AI models and customizing them with users' own data to create custom Computer Vision (CV) and Conversational AI models. This notebook shows an example use case of YOLO v4 object detection using TAO.\n",
    "\n",
    "<img src=\"images/tao_toolkit.jpeg\" width=\"720\">\n",
    "<div style=\"font-size:11px\">Source: https://developer.nvidia.com</div><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the versions of the TAO launcher\n",
    "!tao info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API key\n",
    "\n",
    "Before TAO can be used, you need to register at [ngc.nvidia.com](https://catalog.ngc.nvidia.com/) and proceed to generate an API key. Below is a step-by-step process to achieve this:\n",
    "- From your browser visit `ngc.nvidia.com`\n",
    "- Click on `Register for NGC`\n",
    "- Click on the `Continue` button where `NVIDIA Account (Use existing or create a new NVIDIA account)` is written\n",
    "- Fill in the required information and register. Thereafter you may proceed to log in with your new account credentials\n",
    "- In the top right corner, click on your username and select `Setup` in the dropdown menu\n",
    "- Proceed and click on the `Get API Key` button\n",
    "- Next, you will find a `Generate API Key` button in the upper right corner. After clicking on this button, a dialog box should appear and you have to click on the `Confirm` button\n",
    "- Finally, copy the generated API key and username and save them somewhere on your local system\n",
    "\n",
    "<img src=\"images/ngc_setup_key.png\" width=\"720\">\n",
    "<img src=\"images/ngc_key.png\" width=\"720\">\n",
    "\n",
    "Your API key represents your credentials:\n",
    "- Used for programmatic interaction (e.g. NGC docker registry `nvcr.io`)\n",
    "- Uniquely identifies you (think of it as \"username & password\")\n",
    "- There can only be one (regenerating your API key invalidates the old one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up env variables and map drives\n",
    "\n",
    "When installed, the TAO launcher CLI abstracts the user from having to instantiate and run several docker containers and maps the commands accordingly. However, since the launcher uses docker containers under the hood, drives need to be mapped to the docker. The launcher instance can be configured in the `~/.tao_mounts.json` file.\n",
    "\n",
    "<img src=\"images/tao_tf_user_interaction.png\" width=\"720\">\n",
    "<div style=\"font-size:11px\">Source: https://docs.nvidia.com/tao/tao-toolkit</div><br>\n",
    "\n",
    "When using the purpose-built pretrained models from [NGC](https://catalog.ngc.nvidia.com/), please make sure to set the `$KEY` environment variable to the key as mentioned in the model overview. Failing to do so, can lead to errors when trying to load them as pretrained models.\n",
    "\n",
    "The following notebook requires the user to set an env variable called the `$LOCAL_PROJECT_DIR` as the path to the user's workspace. Please note that the dataset to run this notebook is expected to reside in the `$LOCAL_PROJECT_DIR/data`, the sample spec files are expected to be present in `$LOCAL_PROJECT_DIR/specs`, while the TAO experiment generated collaterals will be output to `$LOCAL_PROJECT_DIR/yolo_v4`. More information on how to set up the dataset and the supported steps in the TAO workflow are provided in the subsequent cells.\n",
    "\n",
    "Please make sure to remove any stray artifacts/files from the `$USER_EXPERIMENT_DIR` or `$DATA_DOWNLOAD_DIR` paths as mentioned below, that may have been generated from previous experiments. Having checkpoint files etc may interfere with creating a training graph for a new experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up env variables for cleaner command line commands\n",
    "import os\n",
    "\n",
    "print(\"Please replace the variable with your key.\")\n",
    "%env KEY=nvidia_tlt\n",
    "\n",
    "# If using a virtual environment and Docker, please define the local project directory that needs to be mapped to the TAO docker session.\n",
    "# The dataset is expected to be present in $LOCAL_PROJECT_DIR/data, while the results from the steps\n",
    "# in this notebook will be stored at $LOCAL_PROJECT_DIR/yolo_v4\n",
    "# The sample spec files are expected to be present in $LOCAL_PROJECT_DIR/specs\n",
    "\n",
    "# Singularity, please do not modify\n",
    "%env LOCAL_PROJECT_DIR=/workspace/tao-experiments\n",
    "# Virtual environment + Docker, set full path to the local workspace\n",
    "#%env LOCAL_PROJECT_DIR=~/end_to_end_CV/workspace\n",
    "\n",
    "# Paths inside the container, please do not modify\n",
    "%env DATA_DOWNLOAD_DIR=/workspace/tao-experiments/data\n",
    "%env USER_EXPERIMENT_DIR=/workspace/tao-experiments/yolo_v4\n",
    "%env SPECS_DIR=/workspace/tao-experiments/specs\n",
    "\n",
    "# Local paths, if using Docker please set the LOCAL_PROJECT_DIR variable above\n",
    "os.environ[\"LOCAL_DATA_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"data\")\n",
    "os.environ[\"LOCAL_EXPERIMENT_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"yolo_v4\")\n",
    "os.environ[\"LOCAL_SPECS_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"specs\")\n",
    "\n",
    "# Showing list of specification files.\n",
    "!ls -rlt $LOCAL_SPECS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local dir\n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR\n",
    "!mkdir -p $LOCAL_PROJECT_DIR/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below maps the project directory on your local host to a workspace directory in the TAO docker instance, so that the data and the results are mapped from outside to inside of the docker instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker\n",
    "import json\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "\n",
    "# Define the dictionary with the mapped drives\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "        # Mapping the data directory\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "            \"destination\": \"/workspace/tao-experiments\"\n",
    "        },\n",
    "        # Mapping the specs directory\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_SPECS_DIR\"],\n",
    "            \"destination\": os.environ[\"SPECS_DIR\"]\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Writing the mounts file\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(drive_map, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the mounts file\n",
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline data augmentation\n",
    "\n",
    "The success of an object detection application is highly dependent on the quality of the data. Acquiring curated and annotated datasets can be very expensive, tiring, and time-consuming. Furthermore, it is very difficult to estimate all the corner cases that a network may go through. \n",
    "\n",
    "Online augmentation in the training data loader is a good way to increase the variation in the dataset and the overall performance. However, the augmented data is generated randomly and in order to achieve good accuracy, the model may need to be trained for a long time. To get around this and generate a dataset with the required augmentations and give full control to the user, TAO Toolkit provides an offline augmentation tool called `augment`. Offline augmentation can dramatically increase the size of the dataset when collecting and labeling data is expensive or not possible. The `augment` tool provides several custom GPU accelerated augmentation routines categorized into:\n",
    "- **Spatial augmentation**\n",
    "- **Color space augmentation**\n",
    "- **Image blur**\n",
    "\n",
    "Spatial augmentation comprises routines like `Rotate`, `Resize`, `Translate`, `Shear`, and `Flip` whereas color space augmentation supports `Hue rotation`, `Brightness offset`, and `Contrast shift`. Along with these augmentation operations, `augment` also enables users to blur images, using a Gaussian blur operator.\n",
    "\n",
    "All augmentation routines currently provided with `augment` are supported only for an object detection dataset. The spatial augmentation routines are applied to the images as well as the labeled data coordinates, while the color augmentation routines and channel-wise blur operator are applied only to images as the object labels are not affected. The sample workflow of using `augment` fits into the general TAO pipeline diagram as follows:\n",
    "\n",
    "<img src=\"images/augmenting.png\" width=\"720\">\n",
    "<div style=\"font-size:11px\">Source: https://docs.nvidia.com/tao/tao-toolkit</div><br>\n",
    "\n",
    "The data is expected to be in KITTI format. The following sections detail how to configure and use the augmentation tool.\n",
    "\n",
    "### Configuring the augmentor\n",
    "\n",
    "The augmentor has several components which the user can configure by using a simple protobuf-based configuration file. The configuration file is made up of four main blocks:\n",
    "- **Spatial augmentation config**\n",
    "- **Color augmentation config**\n",
    "- **Blur config**\n",
    "- **Data dimensions**\n",
    "\n",
    "which have to be specified with nested protobuf elements and global parameters. Let's take a quick overview of each component to understand how to create a configuration file from scratch. Configuration files are required for most TAO operations and therefore will be a recurring pattern throughout the notebook.\n",
    "\n",
    "#### Spatial augmentation config\n",
    "\n",
    "[Spatial augmentation config](https://docs.nvidia.com/tao/tao-toolkit/text/offline_data_augmentation.html#spatial-augmentation-config) contains parameters to configure the spatial augmentation routines. `spatial_config` is a nested protobuf element containing protobuf elements for all the supported spatial augmentation operations, namely `rotation_config`, `flip_config`, `translation_config`, `shear_config`. How to configure each of them is explained in the dedicated part in the documentation linked above. If you don’t wish to introduce any of the supported augmentation operations, just omit the corresponding field. When defining multiple proto elements, it implies that all the augmentation operations are cascaded.\n",
    "\n",
    "Here is an example configuration file for `spatial_config` that augments the image by:\n",
    "\n",
    "1. Flipping along the horizontal axis\n",
    "2. Rotating an image by 10 degrees\n",
    "3. Translating along y-axis by 20 pixels\n",
    "\n",
    "```python\n",
    "# Spatial augmentation config\n",
    "spatial_config{\n",
    "  flip_config{\n",
    "    flip_horizontal: true\n",
    "  }\n",
    "  rotation_config{\n",
    "    angle: 10.0\n",
    "    units: \"degrees\"\n",
    "  }\n",
    "  translation_config{\n",
    "    translate_y: 20\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Color augmentation config\n",
    "\n",
    "[Color augmentation config](https://docs.nvidia.com/tao/tao-toolkit/text/offline_data_augmentation.html#color-augmentation-config) contains parameters to configure the color space augmentation routines. This is a nested protobuf element called `color_config` containing protobuf elements for all the color augmentation operations, namely `hue_saturation_config`, `contrast_config`, `brightness_config`. For more information on how to configure the parameters, please refer to the dedicated part in the documentation linked just above.\n",
    "\n",
    "Here is an example configuration file for `color_config` that augments the image by:\n",
    "\n",
    "1. Applying hue rotation and color saturation augmentation\n",
    "2. Applying a channel-wise brightness shift\n",
    "\n",
    "```python\n",
    "# Color augmentation config\n",
    "color_config{\n",
    "  hue_saturation_config{\n",
    "    hue_rotation_angle: 10.0\n",
    "    saturation_shift: 1.0\n",
    "  }\n",
    "  brightness_config{\n",
    "    offset: 10\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Blur config\n",
    "\n",
    "The `blur_config` protobuf element configures the channel-wise [Gaussian blur](https://docs.nvidia.com/tao/tao-toolkit/text/offline_data_augmentation.html#blur-config) operator to an image. A Gaussian kernel is formulated based on the parameters `size` and `std` and then a 2D convolution is performed between the image and kernel per channel.\n",
    "\n",
    "Here is an example configuration file for `blur_config` that augments the image by applying a Gaussian blur over a 5x5 square:\n",
    "\n",
    "```python\n",
    "# Blur config\n",
    "blur_config{\n",
    "  size: 5\n",
    "  std: 1.0\n",
    "}\n",
    "```\n",
    "\n",
    "#### Data dimensions\n",
    "\n",
    "The last component the user can configure for the augmentor is the output data size. An example configuration file is shown below:\n",
    "\n",
    "```python\n",
    "# Data dimensions\n",
    "output_image_width: 640\n",
    "output_image_height: 384\n",
    "output_image_channel: 3\n",
    "image_extension: \".png\"\n",
    "```\n",
    "\n",
    "### Generate the augmentation spec file\n",
    "\n",
    "By chaining all the pieces together, we get a complete configuration file for augmenting images with `augment`. Run the cell below to view one that summarizes everything we've seen so far. If you want to modify the configuration hyperparameters or just experiment with different augmentations, you can access the file in the `spec` folder by searching for it in the upper left part of JupyterLab or by clicking [here](../specs/default_spec.txt). Please remember to save the file with `ctrl s` after each desired modification and then rerun the cell below to see if the changes have been reflected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $LOCAL_SPECS_DIR/default_spec.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment the dataset\n",
    "\n",
    "Once the configuration file has been generated, the TAO `augment` tool is invoked with a simple command line interface. You will see that this is also a recurring pattern for TAO throughout the notebook. The command to launch the `augment` tool expects the following parameters:\n",
    "- `-d`: the path to the detection dataset\n",
    "- `-a`: the path to augmentation spec file\n",
    "- `-o`: the path to the augmented output dataset\n",
    "- `-v`: optional flag to get detailed logs during the augmentation process\n",
    "\n",
    "Please pay attention to the type of paths required by the TAO command below - these are paths accessible inside of the TAO docker instance that were previously mapped. In this notebook, local paths are recognizable by the prefix `LOCAL` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao augment -d $DATA_DOWNLOAD_DIR/training \\\n",
    "             -a $SPECS_DIR/default_spec.txt \\\n",
    "             -o $DATA_DOWNLOAD_DIR/augmented_dataset \\\n",
    "#             -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize augmented results\n",
    "\n",
    "Now that the dataset has been augmented, it is worthwhile to render the augmented images and labels. The outputs of `augment` are generated in the following paths:\n",
    "- images: `$LOCAL_DATA_DIR/augmented_dataset/image_2`\n",
    "- labels: `$LOCAL_DATA_DIR/augmented_dataset/label_2`\n",
    "\n",
    "If you would like to visualize images with overlain bounding boxes, then please run the cell above with the optional `-v` flag enabled. This generates annotated outputs at:\n",
    "- annotated images: `$LOCAL_DATA_DIR/augmented_dataset/images_annotated`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid visualizer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from math import ceil\n",
    "valid_image_ext = ['.jpg', '.png', '.jpeg', '.ppm']\n",
    "\n",
    "def visualize_images(image_dir, num_cols=4, num_images=10):\n",
    "    output_path = os.path.join(os.environ['LOCAL_DATA_DIR'], image_dir)\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the first 12 images\n",
    "# If you would like to view sample annotated images, then please re-run the augment command with the -v flag\n",
    "# and update the output path below to augmented_dataset/images_annotated\n",
    "OUTPUT_PATH = 'augmented_dataset/image_2' # relative path from $LOCAL_DATA_DIR\n",
    "COLS = 4 # number of columns in the visualizer grid\n",
    "IMAGES = 12 # number of images to visualize\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset and pre-trained model\n",
    "\n",
    "For this tutorial, we will be using the fruit dataset in KITTI format from the previous notebook. The dataset is already augmented and structured to have:\n",
    "- training images in `$LOCAL_DATA_DIR/training/image_2`\n",
    "- training labels in `$LOCAL_DATA_DIR/training/label_2`\n",
    "- testing images in `$LOCAL_DATA_DIR/testing/image_2`\n",
    " \n",
    "You may use this notebook with your own dataset as well. To use this example with your own dataset, please follow the same directory structure as mentioned above. Note that there are no labels for the testing images, so we only use them just to visualize inferences for the trained model.\n",
    "\n",
    "Let's check that the number of images in the directories is what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data\n",
    "DATA_DIR = os.environ.get('LOCAL_DATA_DIR')\n",
    "num_training_images = len(os.listdir(os.path.join(DATA_DIR, \"training/image_2\")))\n",
    "num_training_labels = len(os.listdir(os.path.join(DATA_DIR, \"training/label_2\")))\n",
    "num_testing_images = len(os.listdir(os.path.join(DATA_DIR, \"testing/image_2\")))\n",
    "print(\"Number of images in the train/val set. {}\".format(num_training_images))\n",
    "print(\"Number of labels in the train/val set. {}\".format(num_training_labels))\n",
    "print(\"Number of images in the test set. {}\".format(num_testing_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example of a kitti label in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $LOCAL_DATA_DIR/training/label_2/0001.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the validation dataset out of the training dataset by sampling 10% of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../source_code/N2/generate_val_dataset.py --input_image_dir=$LOCAL_DATA_DIR/training/image_2 \\\n",
    "                                                   --input_label_dir=$LOCAL_DATA_DIR/training/label_2 \\\n",
    "                                                   --output_dir=$LOCAL_DATA_DIR/val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, if you have your own dataset already in a volume (or folder), you can mount the volume on `LOCAL_DATA_DIR` (or create a soft link). Below shows an example:\n",
    "```bash\n",
    "# if your dataset is in /dev/sdc1\n",
    "mount /dev/sdc1 $LOCAL_DATA_DIR\n",
    "\n",
    "# if your dataset is in folder /var/dataset\n",
    "ln -sf /var/dataset $LOCAL_DATA_DIR\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate anchor shape\n",
    "\n",
    "If you use your own dataset, you will need to run the code below to generate the best anchor shape. The anchor shape should match most ground truth boxes in the dataset to help the network learn more precise bounding boxes. YOLOv4 uses this information to capture and incorporate in advance the scale and aspect ratio of specific object classes we want to detect.\n",
    "\n",
    "The anchor shape generated by this script is sorted. Later on, write the first 3 lines into `small_anchor_shape` in the config files, write the middle 3 into `mid_anchor_shape`, and write the last 3 into `big_anchor_shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tao yolo_v4 kmeans -l $DATA_DOWNLOAD_DIR/training/label_2 \\\n",
    "#                     -i $DATA_DOWNLOAD_DIR/training/image_2 \\\n",
    "#                     -n 9 \\\n",
    "#                     -x 640 \\\n",
    "#                     -y 384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate TFRecords\n",
    "\n",
    "YOLOv4 supports two data formats: the sequence format (images folder and raw labels folder with KITTI format) and the tfrecords format (images folder and TFRecords). If you prefer to use the sequence data format during training, you can skip this section. To use sequence data format, please use spec file `yolo_v4_train_resnet18_kitti_seq.txt` and `yolo_v4_retrain_resnet18_kitti_seq.txt` later on. You can check the [documentation](https://docs.nvidia.com/tao/tao-toolkit/text/object_detection/yolo_v4.html#dataset-config) for more details about tfrecords generation and sequence data format usage.\n",
    "\n",
    "To generate TFRecords for YOLOv4 training, use the `dataset_convert` command with arguments:\n",
    "- `-d`: path to the dataset spec file\n",
    "- `-o`: path to the output TFRecords file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao yolo_v4 dataset_convert -d $SPECS_DIR/yolo_v4_tfrecords_kitti_train.txt \\\n",
    "                             -o $DATA_DOWNLOAD_DIR/training/tfrecords/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao yolo_v4 dataset_convert -d $SPECS_DIR/yolo_v4_tfrecords_kitti_val.txt \\\n",
    "                             -o $DATA_DOWNLOAD_DIR/val/tfrecords/val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pre-trained model\n",
    "\n",
    "We will use NGC CLI to get the pre-trained models. For more details, go to [ngc.nvidia.com](https://catalog.ngc.nvidia.com/) and click on `Setup` in the navigation bar. To view all the backbones that are supported by object detection architecture in TAO, run the cell below. We will use a pretrained resnet18 model for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model list nvidia/tao/pretrained_object_detection:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/pretrained_resnet18/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we download the pretrained resnet18 model from NGC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull pretrained model from NGC\n",
    "!ngc registry model download-version nvidia/tao/pretrained_object_detection:resnet18 \\\n",
    "                    --dest $LOCAL_EXPERIMENT_DIR/pretrained_resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check that model is downloaded into dir.\")\n",
    "!ls -l $LOCAL_EXPERIMENT_DIR/pretrained_resnet18/pretrained_object_detection_vresnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide training specification\n",
    "\n",
    "TAO Toolkit requires a configuration spec file in order to train any model. For YOLOv4, it has six major components: `yolov4_config`, `training_config`, `eval_config`, `nms_config`, `augmentation_config`, and `dataset_config`. The format of the spec file is a protobuf text (prototxt) message, and each of its fields can be either a basic data type or a nested message. More information on how to configure each of these protobufs can be found in the [TAO YOLOv4 documentation](https://docs.nvidia.com/tao/tao-toolkit/text/object_detection/yolo_v4.html#creating-a-configuration-file). \n",
    "\n",
    "The main parameters to be modified during the experimental phase concern:\n",
    "- Augmentation parameters for on-the-fly data augmentation\n",
    "- Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc.\n",
    "- Whether to use quantization aware training (QAT) or not\n",
    "\n",
    "With the next command, we provide the pretrained model path on-the-fly, writing our experiment directory in the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i 's,EXPERIMENT_DIR,'\"$USER_EXPERIMENT_DIR\"',' $LOCAL_SPECS_DIR/yolo_v4_train_resnet18_kitti.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the provided sample spec files (`yolo_v4_train_resnet18_kitti.txt` & `yolo_v4_retrain_resnet18_kitti.txt`) disable QAT training. To enable QAT training on a sample spec file, uncomment and run the following lines. QAT emulates the inference time quantization during training, allowing the model to adapt and mitigate in advance the quantization error on weights and tensors when an actual quantized model is generated. The benefit of QAT training is usually a better accuracy when doing INT8 inference with TensorRT compared with traditional calibration-based INT8 TensorRT inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sed -i \"s/enable_qat: false/enable_qat: true/g\" $LOCAL_SPECS_DIR/yolo_v4_train_resnet18_kitti.txt\n",
    "# !sed -i \"s/enable_qat: false/enable_qat: true/g\" $LOCAL_SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommended workflow for training a quantization aware model is depicted in the diagram below:\n",
    "\n",
    "<img src=\"images/tao_cv_qat_workflow.png\" width=\"720\">\n",
    "<div style=\"font-size:11px\">Source: https://docs.nvidia.com/tao/tao-toolkit</div><br>\n",
    "\n",
    "You can restore non-QAT training by uncommenting and running the lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sed -i \"s/enable_qat: true/enable_qat: false/g\" $LOCAL_SPECS_DIR/yolo_v4_train_resnet18_kitti.txt\n",
    "# !sed -i \"s/enable_qat: true/enable_qat: false/g\" $LOCAL_SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to view the model spec configuration file. If you want to modify the configuration hyperparameters, you can access the file in the `spec` folder by searching for it in the upper left part of JupyterLab. Please remember to save the file with `ctrl s` after each desired modification and then rerun the cell below to see if the changes have been reflected. If you generated the best anchor shape for your dataset, go ahead and edit the config file now.\n",
    "\n",
    "Note that in the spec file `arch` is set to `\"resnet\"` as the backbone for feature extraction. Other choices include `“vgg”`, `“darknet”`, `“googlenet”`, `“mobilenet_v1”`, `“mobilenet_v2”`, `“cspdarknet”`, and `“squeezenet”`, but you will need to download the corresponding model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $LOCAL_SPECS_DIR/yolo_v4_train_resnet18_kitti.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run TAO training\n",
    "\n",
    "To launch training, provide the required sample spec file, the output directory location for models, and the encryption key to decrypt the model. Please note some important parameter definitions: \n",
    "- `-e`: the experiment specification file to set up the evaluation experiment\n",
    "- `-r`: the path to the folder where the experiment output is written\n",
    "- `-k`: the encryption key to decrypt the model\n",
    "- `--gpus`: the number of GPUs to use for training\n",
    "\n",
    "Additional optional parameters are available [here](https://docs.nvidia.com/tao/tao-toolkit/text/object_detection/yolo_v4.html#training-the-model). In particular, TAO Toolkit now supports training with automatic mixed precision (AMP), speeding up math-intensive operations and memory-limited operations while not compromising accuracy. Enabling AMP is as simple as setting the `--use_amp` flag at the command line when running the `train` command. This will help speed up the training by using FP16 tensor cores. Note that AMP is only supported on GPUs with Volta or above architecture.\n",
    "\n",
    "Please be aware that depending on the task, training may take several hours or one day to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/experiment_dir_unpruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"To run with multigpu, please change --gpus based on the number of available GPUs in your machine.\")\n",
    "!tao yolo_v4 train -e $SPECS_DIR/yolo_v4_train_resnet18_kitti.txt \\\n",
    "                   -r $USER_EXPERIMENT_DIR/experiment_dir_unpruned \\\n",
    "                   -k $KEY \\\n",
    "                   --gpus 1 \\\n",
    "                   --use_amp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to resume training from a checkpoint, please change `pretrain_model_path` to `resume_model_path` in the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model for each epoch:\")\n",
    "print(\"---------------------\")\n",
    "!ls -ltrh $LOCAL_EXPERIMENT_DIR/experiment_dir_unpruned/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the evaluation stats in the csv file and pick the model with highest eval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $LOCAL_EXPERIMENT_DIR/experiment_dir_unpruned/yolov4_training_log_resnet18.csv\n",
    "%set_env EPOCH=030"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate trained model\n",
    "\n",
    "Once training is complete and a selected model is available, it can be evaluated from the command line by providing the experiment spec file, the encryption key, and the path to the model file with the `-m` flag. Evaluation can only run on a single GPU, so when the machine has multiple GPUs installed, the optional `--gpu_index` flag specifies the GPU index to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tao yolo_v4 evaluate -e $SPECS_DIR/yolo_v4_train_resnet18_kitti.txt \\\n",
    "                      -m $USER_EXPERIMENT_DIR/experiment_dir_unpruned/weights/yolov4_resnet18_epoch_$EPOCH.tlt \\\n",
    "                      -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune trained model\n",
    "\n",
    "Ease of model pruning is one of the key differentiators for the TAO Toolkit. Pruning involves removing from the neural network nodes that contribute less to the overall accuracy, reducing the size of the model, significantly reducing its memory footprint, and increasing inference throughput. All these factors are very important for edge deployment and fast real-time execution at high frame rates.\n",
    "\n",
    "<img src=\"images/pruned_vs_unpruned.png\" width=\"720\">\n",
    "<div style=\"font-size:11px\">Source: https://docs.nvidia.com/tao/tao-toolkit</div><br>\n",
    "\n",
    "The `tao yolo_v4 prune` command includes these parameters:\n",
    "- `-m`: path to pretrained YOLOv4 model\n",
    "- `-e`: path to the experiment specification file\n",
    "- `-o`: output directory to store the pruned model\n",
    "- `-k`: the key to save and load the model\n",
    "- `-eq`: equalization criterion (only for ResNets as they have element-wise operations or MobileNets)\n",
    "- `-pth`: threshold for pruning\n",
    "\n",
    "Usually, you just need to adjust `-pth` (threshold) for accuracy and model size trade-off. Higher `pth` gives you a smaller model (and thus higher inference speed) but worse accuracy. The threshold value depends on the dataset and the model: `0.4` in the block below is just a starting point. If the retrain accuracy is good, you can increase this value to get smaller models. Otherwise, lower this value to get better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/experiment_dir_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tao yolo_v4 prune -m $USER_EXPERIMENT_DIR/experiment_dir_unpruned/weights/yolov4_resnet18_epoch_$EPOCH.tlt \\\n",
    "                   -e $SPECS_DIR/yolo_v4_train_resnet18_kitti.txt \\\n",
    "                   -o $USER_EXPERIMENT_DIR/experiment_dir_pruned/yolov4_resnet18_pruned.tlt \\\n",
    "                   -eq intersection \\\n",
    "                   -pth 0.4 \\\n",
    "                   -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/experiment_dir_pruned/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Retrain pruned model\n",
    "\n",
    "Once the model has been pruned, there might be a slight decrease in accuracy because some previously useful weights may have been removed. The model needs to be retrained on the same dataset to restore the lost accuracy. Therefore, a new retraining specification file must be specified. Please be aware that depending on the task, retraining may take several hours or one day to complete.\n",
    "\n",
    "Here we show the content of the retrain spec file. The newly pruned model has been included as pretrained weights. If you generated the anchor shape for your dataset, please remember to update it in the retrain spec file as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing the retrain spec file\n",
    "# Here we have updated the spec file to include the newly pruned model as pretrained weights\n",
    "!sed -i 's,EXPERIMENT_DIR,'\"$USER_EXPERIMENT_DIR\"',' $LOCAL_SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt\n",
    "!cat $LOCAL_SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/experiment_dir_retrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is possible to retrain using the pruned model as pretrained weights. To do this, the `tao yolo_v4 train` command is invoked again with an updated spec file that points to the newly pruned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao yolo_v4 train -e $SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt \\\n",
    "                   -r $USER_EXPERIMENT_DIR/experiment_dir_retrain \\\n",
    "                   -k $KEY \\\n",
    "                   --gpus 1 \\\n",
    "                   --use_amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the newly retrained model\n",
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/experiment_dir_retrain/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the evaluation stats in the csv file and pick the model with highest eval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $LOCAL_EXPERIMENT_DIR/experiment_dir_retrain/yolov4_training_log_resnet18.csv\n",
    "%set_env EPOCH=015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate retrained model\n",
    "\n",
    "Once retraining is complete, the new pruned version of the model can be assessed as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao yolo_v4 evaluate -e $SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt \\\n",
    "                      -m $USER_EXPERIMENT_DIR/experiment_dir_retrain/weights/yolov4_resnet18_epoch_$EPOCH.tlt \\\n",
    "                      -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize inferences\n",
    "\n",
    "In this section, we run the inference tool for YOLOv4 networks to display the results in the form of bounding boxes for a subset of images. The `inference` command requires to specify the following parameters:\n",
    "- `-i`: the directory of input images for inference\n",
    "- `-o`: the directory path to output annotated images\n",
    "- `-e`: path to the experiment spec file used for training\n",
    "- `-m`: the path to the trained model (TAO model) or TensorRT engine\n",
    "- `-k`: the key to load model (not needed if the model is a TensorRT engine)\n",
    "\n",
    "In addition, predicted labels in KITTI format can be saved to an output label directory with the flag `-l` and the GPU to run inference on can be specified with the flag `--gpu_index`. Also, the `-t` flag sets the confidence threshold for drawing a bounding box and is set to `0.3` by default.\n",
    "\n",
    "Below, we copy some test images to see our model in action on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy some test images\n",
    "!mkdir -p $LOCAL_DATA_DIR/test_samples\n",
    "!cp $LOCAL_DATA_DIR/testing/image_2/00* $LOCAL_DATA_DIR/test_samples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then run inference for detection on these images using the `inference` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao yolo_v4 inference -i $DATA_DOWNLOAD_DIR/test_samples \\\n",
    "                       -o $USER_EXPERIMENT_DIR/yolo_infer_images \\\n",
    "                       -e $SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt \\\n",
    "                       -m $USER_EXPERIMENT_DIR/experiment_dir_retrain/weights/yolov4_resnet18_epoch_$EPOCH.tlt \\\n",
    "                       -l $USER_EXPERIMENT_DIR/yolo_infer_labels \\\n",
    "                       -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `inference` tool call produces two outputs:\n",
    "- Overlain images in `$LOCAL_EXPERIMENT_DIR/yolo_infer_images`\n",
    "- Frame by frame bbox labels in kitti format located in `$LOCAL_EXPERIMENT_DIR/yolo_infer_labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid visualizer\n",
    "valid_image_ext = ['.jpg', '.png', '.jpeg', '.ppm']\n",
    "\n",
    "def visualize_images(image_dir, num_cols=4, num_images=10):\n",
    "    output_path = os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'], image_dir)\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in sorted(os.listdir(output_path)) \n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images\n",
    "OUTPUT_PATH = 'yolo_infer_images' # relative path from $USER_EXPERIMENT_DIR\n",
    "COLS = 3 # number of columns in the visualizer grid\n",
    "IMAGES = 9 # number of images to visualize\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model export\n",
    "\n",
    "Exporting the model decouples the training process from inference and allows conversion to TensorRT engines outside the TAO environment. TensorRT engines are specific to each hardware configuration and should be generated for each unique inference environment, while the exported model may be used universally across training and deployment hardware. The exported model is in `.etlt` format and is encrypted with the same key as the `.tlt` model from which it was exported. The key is required when deploying this model.\n",
    "\n",
    "If you trained a non-QAT model, you may export in `FP32`, `FP16`, or `INT8` mode using the code block below. For `INT8`, you need to provide a calibration image directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tao <task> export will fail if .etlt already exists. So we clear the export folder before tao <task> export\n",
    "!rm -rf $LOCAL_EXPERIMENT_DIR/export\n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/export\n",
    "# Export in FP32 mode. Change --data_type to fp16 for FP16 mode\n",
    "!tao yolo_v4 export -m $USER_EXPERIMENT_DIR/experiment_dir_retrain/weights/yolov4_resnet18_epoch_$EPOCH.tlt \\\n",
    "                    -k $KEY \\\n",
    "                    -o $USER_EXPERIMENT_DIR/export/yolov4_resnet18_epoch_$EPOCH.etlt \\\n",
    "                    -e $SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt \\\n",
    "                    --batch_size 16 \\\n",
    "                    --data_type fp32 \\\n",
    "                    --gen_ds_config\n",
    "\n",
    "# Uncomment to export in INT8 mode (generate calibration cache file). \n",
    "# !tao yolo_v4 export -m $USER_EXPERIMENT_DIR/experiment_dir_retrain/weights/yolov4_resnet18_epoch_$EPOCH.tlt  \\\n",
    "#                     -o $USER_EXPERIMENT_DIR/export/yolov4_resnet18_epoch_$EPOCH.etlt \\\n",
    "#                     -e $SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt \\\n",
    "#                     -k $KEY \\\n",
    "#                     --cal_image_dir $DATA_DOWNLOAD_DIR/testing/image_2 \\\n",
    "#                     --data_type int8 \\\n",
    "#                     --batch_size 16 \\\n",
    "#                     --batches 10 \\\n",
    "#                     --cal_cache_file $USER_EXPERIMENT_DIR/export/cal.bin  \\\n",
    "#                     --cal_data_file $USER_EXPERIMENT_DIR/export/cal.tensorfile \\\n",
    "#                     --gen_ds_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this example, for ease of execution we restrict the number of calibrating batches to 10. TAO Toolkit recommends using at least 10% of the training dataset for int8 calibration.\n",
    "\n",
    "If you train a QAT model, you may only export in `INT8` mode using the following code block. This generates a `.etlt` file and the corresponding calibration cache. You can throw away the calibration cache and just use the `.etlt` file in tao-converter or DeepStream for `FP32` or `FP16` mode, but please note this gives sub-optimal results. If you want to deploy in `FP32` or `FP16`, you should disable QAT in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to export QAT model in INT8 mode (generate calibration cache file).\n",
    "# !rm -rf $LOCAL_EXPERIMENT_DIR/export\n",
    "# !mkdir -p $LOCAL_EXPERIMENT_DIR/export\n",
    "# !tao yolo_v4 export -m $USER_EXPERIMENT_DIR/experiment_dir_retrain/weights/yolov4_resnet18_epoch_$EPOCH.tlt  \\\n",
    "#                     -o $USER_EXPERIMENT_DIR/export/yolov4_resnet18_epoch_$EPOCH.etlt \\\n",
    "#                     -e $SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt \\\n",
    "#                     -k $KEY \\\n",
    "#                     --data_type int8 \\\n",
    "#                     --cal_cache_file $USER_EXPERIMENT_DIR/export/cal.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exported model:\")\n",
    "print(\"------------\")\n",
    "!ls -lh $LOCAL_EXPERIMENT_DIR/export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TensorRT engine\n",
    "\n",
    "TAO Toolkit has been designed to integrate with Triton Inference Server, an open-source inference serving software, as well as DeepStream SDK, a streaming toolkit to accelerate building and deploying AI-based video analytic applications that we will discuss in a later notebook. To deploy a model trained with TAO Toolkit to DeepStream we have two options:\n",
    "- **Option 1:** Integrate the `.etlt` model and calibration cache (for int8 mode) generated by `export` directly to DeepStream to automatically create the TensorRT engine file and then run inference on your target device\n",
    "- **Option 2:** Generate a device-specific, optimized TensorRT engine ahead of time using `tao-converter`, that can also be ingested directly by DeepStream\n",
    "\n",
    "In addition to the first option discussed before, in this section, we illustrate how to execute the second one as well, as a TensorRT engine will be used for deployment to Triton Inference Server. Please note that **a distinct engine should be generated for each environment and hardware configuration**, so the first option is the preferred one to minimize the chances of encountering problems. In fact, running an engine that was generated with a different version of TensorRT and CUDA is not supported and may fail to run altogether, or cause unknown behavior that affects inference speed, accuracy, and stability.\n",
    "\n",
    "<img src=\"images/dstream_deploy_options.png\" width=\"720\">\n",
    "<div style=\"font-size:11px\">Source: https://docs.nvidia.com/tao/tao-toolkit</div><br>\n",
    "\n",
    "The `tao-converter` utility included with the TAO docker produces optimized TensorRT engines for the platform that it resides on. Therefore, to get maximum performance, please instantiate this docker and execute the `tao-converter` command, with the exported `.etlt` file and calibration cache (for int8 mode) on your target device. The tao-converter utility included in this docker only works for x86 devices, with discrete NVIDIA GPUs. For the jetson devices, please download the tao-converter for jetson from the dev zone link [here](https://developer.nvidia.com/tao-converter). \n",
    "\n",
    "The `-p` argument in the following command is the optimization profile for `.etlt` models with dynamic shape. This should be in format `<input_node>,<min_shape>,<opimal_shape>,<max_shape>`, where each shape has the format `<n>x<c>x<h>x<w>`. In YOLOv4, the three shapes should only have differences at the first batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorRT engine (FP32)\n",
    "!tao converter -k $KEY \\\n",
    "                   -p Input,1x3x384x640,8x3x384x640,16x3x384x640 \\\n",
    "                   -e $USER_EXPERIMENT_DIR/export/trt.engine \\\n",
    "                   -t fp32 \\\n",
    "                   $USER_EXPERIMENT_DIR/export/yolov4_resnet18_epoch_$EPOCH.etlt\n",
    "\n",
    "# Convert to TensorRT engine (FP16)\n",
    "# !tao converter -k $KEY \\\n",
    "#                    -p Input,1x3x384x640,8x3x384x640,16x3x384x640 \\\n",
    "#                    -e $USER_EXPERIMENT_DIR/export/trt.engine \\\n",
    "#                    -t fp16 \\\n",
    "#                    $USER_EXPERIMENT_DIR/export/yolov4_resnet18_epoch_$EPOCH.etlt\n",
    "\n",
    "# Convert to TensorRT engine (INT8).\n",
    "# !tao converter -k $KEY  \\\n",
    "#                    -p Input,1x3x384x640,8x3x384x640,16x3x384x640 \\\n",
    "#                    -c $USER_EXPERIMENT_DIR/export/cal.bin \\\n",
    "#                    -e $USER_EXPERIMENT_DIR/export/trt.engine \\\n",
    "#                    -b 8 \\\n",
    "#                    -t int8 \\\n",
    "#                    $USER_EXPERIMENT_DIR/export/yolov4_resnet18_epoch_$EPOCH.etlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exported engine:\")\n",
    "print(\"------------\")\n",
    "!ls -lh $LOCAL_EXPERIMENT_DIR/export/trt.engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify deployed model \n",
    "\n",
    "We can verify the converted engine by visualizing TensorRT inferences using the `tao yolo_v4 inference` command seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer using TensorRT engine\n",
    "!tao yolo_v4 inference -m $USER_EXPERIMENT_DIR/export/trt.engine \\\n",
    "                       -e $SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt \\\n",
    "                       -i $DATA_DOWNLOAD_DIR/test_samples \\\n",
    "                       -o $USER_EXPERIMENT_DIR/yolo_infer_images \\\n",
    "                       -t 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images\n",
    "OUTPUT_PATH = 'yolo_infer_images' # relative path from $USER_EXPERIMENT_DIR\n",
    "COLS = 3 # number of columns in the visualizer grid\n",
    "IMAGES = 9 # number of images to visualize\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have shown how to increase the dataset size with offline data augmentation and how to perform transfer learning starting from a pretrained resnet18 yolo_v4 model. We covered model pruning and optimization and provided all it is needed to include quantization in the training loop as well. Finally, we exported our best model first into a .etlt file for deployment to DeepStream, and a TensorRT engine for deployment on Triton Inference Server. We will explore both of these deployment options in detail going forward.\n",
    "\n",
    "In order to successfully continue the lab, now please go back to the `README` file and follow the instructions there before running the next notebook covering deployment with Triton Inference Server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## References\n",
    "\n",
    "- [1] *https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/resources/cv_samples/version/v1.4.1/files/yolo_v4/yolo_v4.ipynb*\n",
    "- [2] *https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/resources/cv_samples/version/v1.4.1/files/augment/augment.ipynb*\n",
    "\n",
    "## Licensing\n",
    "\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"1.Data_labeling_and_preprocessing.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 34%; text-align: center;\">\n",
    "        <a href=\"1.Data_labeling_and_preprocessing.ipynb\">1</a>\n",
    "        <a >2</a>\n",
    "        <a href=\"3.Model_deployment_with_Triton_Inference_Server.ipynb\">3</a>\n",
    "        <a href=\"4.Model_deployment_with_DeepStream.ipynb\">4</a>\n",
    "        <a href=\"5.Measure_object_size_using_OpenCV.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"3.Model_deployment_with_Triton_Inference_Server.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<p> <center> <a href=\"../Start_here.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
