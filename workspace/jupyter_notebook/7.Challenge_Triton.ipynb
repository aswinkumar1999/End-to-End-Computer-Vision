{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8439ea6f-1733-4d22-bd4d-19c193ddea84",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../Start_here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"6.Challenge_DeepStream.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 34%; text-align: center;\">\n",
    "        <a href=\"1.Data_labeling_and_preprocessing.ipynb\">1</a>\n",
    "        <a href=\"2.Object_detection_using_TAO_YOLOv4.ipynb\">2</a>\n",
    "        <a href=\"3.Model_deployment_with_Triton_Inference_Server.ipynb\">3</a>\n",
    "        <a href=\"4.Model_deployment_with_DeepStream.ipynb\">4</a>\n",
    "        <a href=\"5.Measure_object_size_using_OpenCV.ipynb\">5</a>\n",
    "        <a href=\"6.Challenge_DeepStream.ipynb\">6</a>\n",
    "        <a >7</a>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47202426-f160-4c29-a8f6-063282168535",
   "metadata": {},
   "source": [
    "# Exercise: model deployment with Triton Inference Server\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9e259-6801-48b2-a0be-9a87b5bcb8a1",
   "metadata": {},
   "source": [
    "In this notebook, you will review the concepts learned in [3.Model_deployment_with_Triton_Inference_Server.ipynb](3.Model_deployment_with_Triton_Inference_Server.ipynb) while trying to deploy your NVIDIA® TAO Toolkit model to Triton™ Inference Server and improve performance with inference optimization.\n",
    "\n",
    "As an exercise, you are asked to re-implement the same HTTP and gRPC inference pipelines that have been analyzed in the tutorial notebook.\n",
    "\n",
    "<img src=\"images/triton_inference_server.jpg\" width=\"720\">\n",
    "<div style=\"font-size:11px\">Source: https://developer.nvidia.com/nvidia-triton-inference-server</div><br>\n",
    "\n",
    "Let us get started with the challenge. You will have to fill in the `COMPLETE THIS SECTION` parts of the code present in the notebook to complete the pipelines. Feel free to refer to the previous notebooks for the commands but make sure to grasp the most important underlying concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4cd161-1c41-4322-9cc7-6693c04eb8f5",
   "metadata": {},
   "source": [
    "## Setup server and client\n",
    "\n",
    "**Server**\n",
    "\n",
    "To successfully execute the code in this notebook, you should already have an instance of Triton Inference Server running. Please relaunch the server container following the instructions in the `README` file if you shut it down previously. Remember to use the container in polling mode, so that changes you make to the model repository while running the code cells will be detected periodically and Triton will attempt to load and unload models as necessary based on those changes. If you are using Docker, you can launch the container by running the command below.\n",
    "\n",
    "```\n",
    "docker run \\\n",
    "  --gpus=1 --rm \\\n",
    "  -p 8000:8000 -p 8001:8001 -p 8002:8002 \\\n",
    "  -v /full/path/to/model/repository:/models \\\n",
    "  nvcr.io/nvidia/tritonserver:<yy.mm>-py3 \\\n",
    "  tritonserver \\\n",
    "  --model-repository=/models \\\n",
    "  --exit-on-error=false \\\n",
    "  --model-control-mode=poll \\\n",
    "  --repository-poll-secs 30\n",
    "```\n",
    "\n",
    "The `--gpus=1` flag indicates that 1 system GPU should be made available to Triton for inferencing, while `<yy.mm>` is the version of Triton that you want to use and pull from the NVIDIA Container Toolkit. The path to the model repository needs to be set as well.\n",
    "\n",
    "**Client**\n",
    "\n",
    "The Triton client libraries that provide application programming interfaces (APIs) that make it easy to communicate with Triton from a C++ or Python application have also been installed in the environment from which [3.Model_deployment_with_Triton_Inference_Server.ipynb]( 3.Model_deployment_with_Triton_Inference_Server.ipynb) was executed. Please make sure you are running this exercise from the same virtual environment/container. For any doubt, please follow the instructions in the `README` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0889894d-1094-45b3-b4a4-492f861ed642",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the model repository\n",
    "\n",
    "Triton Inference Server stores available models in the model repository. The directory where the models reside inside the container is specified when starting the server instance using the `tritonserver --model-repository=/models` flag. Each model then resides in its own subdirectory within the main model repository (i.e. each directory within `/models` represents a unique model). For example, in this notebook, we will deploy the TensorRT engine generated from the TAO training in the `yolov4_tao_challenge` subdirectory.\n",
    "\n",
    "The layout of a minimal model repository should look like this:\n",
    "\n",
    "```\n",
    "models\n",
    "└── yolov4_tao_challenge\n",
    "    ├── 1\n",
    "    │   └── model.plan\n",
    "    └── config.pbtxt\n",
    "```\n",
    "\n",
    "For more details on how to work with model repositories and model directory structures in Triton Inference Server, please check the documentation [here](https://github.com/triton-inference-server/server/blob/r22.07/docs/model_repository.md).\n",
    "\n",
    "Below, we'll create the model directory structure for our TensorRT model and copy the engine we generated in the previous [2.Object_detection_using_TAO_YOLOv4.ipynb](2.Object_detection_using_TAO_YOLOv4.ipynb) notebook to the newly prepared folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d30ce-7859-4de2-b011-b78e77984232",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models/yolov4_tao_challenge/1/\n",
    "# Copy the TensorRT engine and rename it to match the default name model.plan\n",
    "!cp ../yolo_v4/export/trt.engine ../models/yolov4_tao_challenge/1/model.plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de33bbff-1392-44ab-a574-39842c24e4b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create configuration file\n",
    "\n",
    "With our TAO model already defined and exported in TensorRT plan representation, we now focus on creating the configuration file that provides required and optional information about the model.\n",
    "\n",
    "A minimal model configuration must specify the platform and/or backend properties, the max_batch_size property, and the input and output tensors of the model (name, data type, and shape). A YOLOv4 model has 1 input node `Input` and 4 output nodes `BatchedNMS`, `BatchedNMS_1`, `BatchedNMS_2` and `BatchedNMS_3`.\n",
    "\n",
    "For more details on how to create model configuration files within Triton Inference Server, please see the documentation [here](https://github.com/triton-inference-server/server/blob/r22.07/docs/model_configuration.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6390ab0c-3d67-4619-a79e-73f6263c339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# ~~~~~~~ COMPLETE THIS SECTION ~~~~~~~ #############\n",
    "configuration = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 16\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "\"\"\"\n",
    "###################### ~~~~~~~ END ~~~~~~~ ######################\n",
    "\n",
    "with open(\"../models/yolov4_tao_challenge/config.pbtxt\", 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f136a4-40f7-4c14-9a76-0b6de9605d7c",
   "metadata": {},
   "source": [
    "## Check loaded model in Triton Inference Server\n",
    "\n",
    "With the model repository created, the TensorRT model defined and exported, and the configuration file written, we will now wait for Triton Inference Server to load our model. This notebook is set to continuously poll for modifications once every 30 seconds, so please run the cell below to ensure enough time has passed before proceeding (15 seconds have been added just to be safe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee54680-bcac-470a-9c6c-01f9f999ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleep 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85d3ee4-4946-4ee2-a35c-575c7209d5f6",
   "metadata": {},
   "source": [
    "At this point, our model should be deployed and ready to use! To confirm Triton Inference Server is up and running, we can see the output of a `curl` request to the below URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b8ccfa-a48e-40f9-9cf1-09746755a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v localhost:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4f353-ee11-4be3-8267-53fa09154804",
   "metadata": {},
   "source": [
    "The HTTP request returns status 200 if Triton is ready and non-200 if it is not ready.\n",
    "\n",
    "We can also send a `curl` request to our model endpoints to confirm our model is deployed and ready to use. This `curl` request returns status 200 if the model is ready and non-200 if it is not ready. \n",
    "\n",
    "Additionally, we will also see information about our model such as:\n",
    "- The name of our model.\n",
    "- The versions available for our model.\n",
    "- The backend platform (e.g. tensorrt_plan).\n",
    "- The inputs and outputs, with their respective names, data types, and shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b681f9-855b-4741-b8d3-0dc46776aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v localhost:8000/v2/models/yolov4_tao_challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff35a6c-c9ba-42d8-9df1-88ae68637c6a",
   "metadata": {},
   "source": [
    "## Send inference request to the server\n",
    "\n",
    "With our model deployed and ready to use, it is now time to send inference requests to it. We'll start by loading the `tritonclient.http` module and defining a set of variables including the name of our model, the URL where it is deployed, the model version, and paths from which to load the images and where to save the processed outputs. Make sure to use the newly created model, not the one loaded for the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719d0603-4989-4fc1-9df4-2d0f7b8e73ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "sys.path.append(\"../source_code/N3\")\n",
    "from utils import convert_http_metadata_config\n",
    "\n",
    "############# ~~~~~~~ COMPLETE THIS SECTION ~~~~~~~ #############\n",
    "verbose =\n",
    "url =\n",
    "model_name =\n",
    "model_version =\n",
    "protocol =\n",
    "batch_size =\n",
    "###################### ~~~~~~~ END ~~~~~~~ ######################\n",
    "image_filename = \"../data/testing/image_2/\"\n",
    "output_path = \"../source_code/challenge_triton/triton_output_http\"\n",
    "\n",
    "import os\n",
    "if not os.path.exists(\"../source_code/challenge_triton\"):\n",
    "    !mkdir ../source_code/challenge_triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98e804c-81e2-40be-b7bf-eba12515f918",
   "metadata": {},
   "source": [
    "Then, we instantiate the Triton Client and get access to additional properties from the model metadata and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adf155a-43b6-454b-b48f-6f7245b2ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running the inference client \\n\")\n",
    "\n",
    "try:\n",
    "    triton_client = httpclient.InferenceServerClient(\n",
    "        url=url, verbose=verbose)\n",
    "except Exception as e:\n",
    "    print(\"client creation failed: \" + str(e))\n",
    "    sys.exit(1)\n",
    "\n",
    "# Make sure the model matches our requirements, and get some\n",
    "# properties of the model that we need for preprocessing\n",
    "try:\n",
    "    model_metadata = triton_client.get_model_metadata(\n",
    "        model_name=model_name, model_version=model_version)\n",
    "except InferenceServerException as e:\n",
    "    print(\"failed to retrieve the metadata: \" + str(e))\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    model_config = triton_client.get_model_config(\n",
    "        model_name=model_name, model_version=model_version)\n",
    "except InferenceServerException as e:\n",
    "    print(\"failed to retrieve the config: \" + str(e))\n",
    "    sys.exit(1)\n",
    "\n",
    "model_metadata, model_config = convert_http_metadata_config(\n",
    "    model_metadata, model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18cbfb1-65d5-4cad-8287-ea36057c0332",
   "metadata": {},
   "source": [
    "Next, we load the model and process the images from our input directory by converting, resizing, and loading them into a data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811cb04-8512-418e-aaf6-c1ec0c1d5e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov4_model import YOLOv4Model\n",
    "from tritonclient.utils import triton_to_np_dtype\n",
    "import os\n",
    "from frame import Frame\n",
    "\n",
    "triton_model = YOLOv4Model.from_metadata(model_metadata, model_config)\n",
    "max_batch_size = triton_model.max_batch_size\n",
    "target_shape = (triton_model.c, triton_model.h, triton_model.w)\n",
    "npdtype = triton_to_np_dtype(triton_model.triton_dtype)\n",
    "\n",
    "print(\"\\nLoading images... \\n\")\n",
    "\n",
    "frames = []\n",
    "\n",
    "if os.path.exists(image_filename):\n",
    "    # The input is a folder of images\n",
    "    if os.path.isdir(image_filename):\n",
    "        frames = [\n",
    "            Frame(os.path.join(image_filename, f),\n",
    "                triton_model.data_format,\n",
    "                npdtype,\n",
    "                target_shape)\n",
    "            for f in os.listdir(image_filename)\n",
    "            if os.path.isfile(os.path.join(image_filename, f)) and\n",
    "            os.path.splitext(f)[-1] in [\".jpg\", \".jpeg\", \".png\"]\n",
    "        ]\n",
    "    # The input is an image\n",
    "    else:\n",
    "        frames = [\n",
    "            Frame(os.path.join(image_filename),\n",
    "                triton_model.data_format,\n",
    "                npdtype,\n",
    "                target_shape)\n",
    "        ]\n",
    "    print(\"Done! \\n\")\n",
    "else:\n",
    "    print(\"No images found, please specify a valid path \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f348c583-ac03-404a-9c6b-37b0622b5600",
   "metadata": {},
   "source": [
    "Finally, we use a request generator to submit our inputs to the Triton Inference Server using the `triton_client.infer()` method, specifying our model name, version, inputs and outputs. The responses we get are stored in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f37d2-11fa-4c0b-9fb8-e74ffaca4ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from utils import requestGenerator\n",
    "import time\n",
    "\n",
    "# Send requests of batch_size images. If the number of\n",
    "# images isn't an exact multiple of batch_size then just\n",
    "# start over with the first images until the batch is filled.\n",
    "\n",
    "print(\"Sending inference request for batches of data \\n\")\n",
    "\n",
    "responses = []\n",
    "image_idx = 0\n",
    "last_request = False\n",
    "sent_count = 0\n",
    "pbar_total = len(frames)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=pbar_total) as pbar:\n",
    "    while not last_request:\n",
    "        batched_image_data = None\n",
    "\n",
    "        repeated_image_data = []\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            frame = frames[image_idx]\n",
    "\n",
    "            img = frame._load_img()\n",
    "            repeated_image_data.append(img)\n",
    "\n",
    "            image_idx = (image_idx + 1) % len(frames)\n",
    "            if image_idx == 0:\n",
    "                last_request = True\n",
    "\n",
    "        if max_batch_size > 0:\n",
    "            batched_image_data = np.stack(repeated_image_data, axis=0)\n",
    "        else:\n",
    "            batched_image_data = repeated_image_data[0]\n",
    "\n",
    "        # Send request\n",
    "        try:\n",
    "            req_gen_args = [batched_image_data, triton_model.input_names,\n",
    "                triton_model.output_names, triton_model.triton_dtype,\n",
    "                protocol.lower()]\n",
    "            req_generator = requestGenerator(*req_gen_args)\n",
    "            for inputs, outputs in req_generator:\n",
    "                sent_count += 1\n",
    "\n",
    "                responses.append(\n",
    "                    triton_client.infer(model_name,\n",
    "                                        inputs,\n",
    "                                        request_id=str(sent_count),\n",
    "                                        model_version=model_version,\n",
    "                                        outputs=outputs))\n",
    "\n",
    "        except InferenceServerException as e:\n",
    "            print(\"inference failed: \" + str(e))\n",
    "            sys.exit(1)\n",
    "        \n",
    "        pbar.update(batch_size)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Average latency: ~{} seconds\".format((end_time - start_time) / sent_count))\n",
    "print(\"Average throughput: ~{} examples / second\".format(batch_size * sent_count / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8077f1-f1a8-480b-a58a-75e72609a6e8",
   "metadata": {},
   "source": [
    "The responses we get need to be decoded and converted to a NumPy array. Fill in the cell below to examine the shapes of a sample output after the conversion to NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3dfbd2-13e6-4071-86f4-bfec5f7cc873",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# ~~~~~~~ COMPLETE THIS SECTION ~~~~~~~ #############\n",
    "sample_output =\n",
    "\n",
    "output_names =\n",
    "output_array =       \n",
    "  \n",
    "for output_name in output_names:\n",
    "    output_array.append( )\n",
    "\n",
    "print([a.shape for a in output_array])\n",
    "###################### ~~~~~~~ END ~~~~~~~ ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5658cca6-b9fb-406e-8eba-a9a10617545c",
   "metadata": {},
   "source": [
    "We recognize the four output shapes of our model but to convert these numbers into an output we can read and visualize, we pass the responses to a specific postprocessor that renders images with bounding boxes at `$output_path/infer_images` and labels in KITTI format at `$output_path/infer_labels`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cdb7e5-5ec3-4302-b960-c9351f22e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov4_postprocessor import YOLOv4Postprocessor\n",
    "\n",
    "print(\"Gathering responses from the server and post-processing the inferenced outputs \\n\")\n",
    "\n",
    "args_postprocessor = [\n",
    "    batch_size, frames, output_path, triton_model.data_format\n",
    "]\n",
    "\n",
    "postprocessor = YOLOv4Postprocessor(*args_postprocessor)\n",
    "\n",
    "processed_request = 0\n",
    "with tqdm(total=len(frames)) as pbar:\n",
    "    while processed_request < sent_count:\n",
    "        response = responses[processed_request]\n",
    "\n",
    "        this_id = response.get_response()[\"id\"]\n",
    "\n",
    "        postprocessor.apply(\n",
    "            response, this_id, render=True\n",
    "        )\n",
    "        processed_request += 1\n",
    "        pbar.update(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a3d48-a8f0-4412-9a1f-97b161b25440",
   "metadata": {},
   "source": [
    "Let's observe the output on the test images to confirm that the model is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8768a60-7187-4fe4-b3d5-bc62ad67ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid visualizer\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "\n",
    "def visualize_images(image_dir, num_cols=4, num_images=10):\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(image_dir, image) for image in os.listdir(image_dir) \n",
    "         if os.path.splitext(image)[1].lower() == '.png']\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img)\n",
    "        \n",
    "# Visualizing the sample images\n",
    "OUTPUT_PATH = os.path.join(output_path, 'infer_images')\n",
    "COLS = 3 # number of columns in the visualizer grid\n",
    "IMAGES = 9 # number of images to visualize\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0de0a1-7812-40a4-9687-6ae3e9c5bdaa",
   "metadata": {},
   "source": [
    "With this, we have successfully run HTTP inference using Triton with our object detection model and rendered the results in a useful format. As you may have noticed, a lot of work is required for preprocessing and postprocessing of the results, while inference itself does not require a lot of code and could be simplified even more. As inference is at the heart of this lab, you are now asked to speed it up even further using inference optimization tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27385e1-7609-48ac-99f5-e7bd614aa1e1",
   "metadata": {},
   "source": [
    "## Improve inference performance\n",
    "\n",
    "Here we quickly go through a list of things to help deliver maximum performance. These include variable batch size, dynamic batching, gRPC protocol, and asynchronous inference.\n",
    "\n",
    "### Variable batch size\n",
    "\n",
    "In our example, we have worked with data inputs that have a batch size of 1. However, we might often want to use different batch sizes such as 4, 8, 16, or even higher. This has a natural tradeoff of latency and throughput. Since our batches are larger, it might take longer to process an individual batch - increasing the latency. However, since the GPU has more data to work with and we're less constrained by networking and I/O, we might see an increase in throughput - or the number of examples that can be processed per second. Depending on the application, this might be a good way to go. Feel free to go back and vary the batch size to see the impact it has on latency and throughput.\n",
    "\n",
    "### Dynamic batching\n",
    "\n",
    "For most models, the Triton feature that provides the largest performance improvement is dynamic batching. This is a feature that allows individual inference requests to be combined by the server, creating batches dynamically. As we said just above, creating a batch of requests typically results in increased throughput since it executes much more efficiently on the GPU. To enable dynamic batching, simply add the following:\n",
    "\n",
    "```\n",
    "dynamic_batching { }\n",
    "```\n",
    "\n",
    "to the model configuration file to enable dynamic batching with all default settings. By default, the dynamic batcher will create batches as large as possible up to the maximum batch size and will not delay when forming batches. \n",
    "\n",
    "This behavior can be modified by specifying the `preferred_batch_size property`, which indicates the batch sizes that the dynamic batcher should attempt to create, and the `max_queue_delay_microseconds`, setting the maximum delay in sending an inference request as is (even if not of a preferred size) when a batch of a preferred size cannot be formed. For more information on this, please check the [model configuration](https://github.com/triton-inference-server/server/blob/r22.07/docs/model_configuration.md) and [model optimization](https://github.com/triton-inference-server/server/blob/r22.07/docs/optimization.md) docs.\n",
    "\n",
    "Below, you can modify our model configuration file so that Triton Inference Server will deploy it using dynamic batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006fb51c-6f91-431f-9635-1d285558662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# ~~~~~~~ COMPLETE THIS SECTION ~~~~~~~ #############\n",
    "configuration = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 16\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "\"\"\"\n",
    "###################### ~~~~~~~ END ~~~~~~~ ######################\n",
    "\n",
    "with open(\"../models/yolov4_tao_challenge/config.pbtxt\", 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d353019e-eb5e-42d7-a427-24c60ac17f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleep 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b1bfc-00fe-4a78-8236-8bf8932cd7c4",
   "metadata": {},
   "source": [
    "### Asynchronous inference\n",
    "\n",
    "So far, our requests have been submitted to Triton Inference Server synchronously. In other words, we submit a request to Triton, which computes and returns the result, and then we submit our next request. However, it is also possible to submit as many requests as possible, allow Triton to queue requests it hasn't yet processed, and return results as soon as they are computed. This paradigm is known as asynchronous inferencing and can result in some impressive speedups for throughput.\n",
    "\n",
    "### gRPC protocol\n",
    "\n",
    "Last but not least, let's spend a couple of words on switching protocol to gRPC. As we discussed, clients can communicate with Triton using either HTTP or gRPC protocol. Most people are familiar with HTTP, which is the backbone of the internet, but gRPC is a newer, open-source remote procedure call system initially developed at Google in 2015 that uses HTTP/2 for transport and protocol buffers as the interface description language. It is highly efficient and using it is very easy: all you need to do is switch to the `tritonclient.grpc.InferenceServerClient` module, change the inference server URL and make other minimal changes to the pipeline. Using a slightly different protocol can have an enormous impact on latency and throughput, so remember that gRPC exists!\n",
    "\n",
    "\n",
    "## Analyze the impact of inference optimization\n",
    "\n",
    "You are now asked to implement the aforementioned strategies in this notebook and see the effect they have on performance. In particular, you will add both asynchronous inference and gRPC protocol to the pipeline. The model configuration file has already been updated to make use of dynamic batching. Let's import the `tritonclient.grpc` module and set the new url for `gRPC` protocol requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe4e05e-c282-47f3-a0b7-fde9ae4f434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as grpcclient\n",
    "\n",
    "############# ~~~~~~~ COMPLETE THIS SECTION ~~~~~~~ #############\n",
    "verbose =\n",
    "url =\n",
    "protocol =\n",
    "batch_size =\n",
    "###################### ~~~~~~~ END ~~~~~~~ ######################\n",
    "output_path = \"../source_code/challenge_triton/triton_output_grpc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4220bc1-b2bb-4a46-bac0-88445432b584",
   "metadata": {},
   "source": [
    "Then, we instantiate the new Triton Client and get access to additional properties from the model metadata and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45eb09c-77a9-40d4-bfc1-e38b2473bf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running the inference client \\n\")\n",
    "\n",
    "try:\n",
    "    # Create gRPC client for communicating with the server\n",
    "    triton_client = grpcclient.InferenceServerClient(\n",
    "        url=url, verbose=verbose)\n",
    "except Exception as e:\n",
    "    print(\"client creation failed: \" + str(e))\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    model_metadata = triton_client.get_model_metadata(\n",
    "        model_name=model_name, model_version=model_version)\n",
    "except InferenceServerException as e:\n",
    "    print(\"failed to retrieve the metadata: \" + str(e))\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    model_config = triton_client.get_model_config(\n",
    "        model_name=model_name, model_version=model_version)\n",
    "except InferenceServerException as e:\n",
    "    print(\"failed to retrieve the config: \" + str(e))\n",
    "    sys.exit(1)\n",
    "\n",
    "model_config = model_config.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6137509f-68f5-4480-83c2-eb15c02ebda0",
   "metadata": {},
   "source": [
    "Images are already loaded so we can go ahead and submit our inputs to the Triton Inference Server using the `triton_client.async_infer()` method, specifying once again our model name, version, inputs and outputs. The responses we get are then stored in an array at the end like before. Below, we also call a utility callback function for handling asynchronous requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a6b00c-0a00-4854-8494-7d51f1289a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from user_data import UserData\n",
    "from functools import partial\n",
    "\n",
    "def completion_callback(user_data, result, error):\n",
    "    \"\"\"Callback function used for async_stream_infer().\"\"\"\n",
    "    user_data._completed_requests.put((result, error))\n",
    "\n",
    "print(\"Sending inference request for batches of data \\n\")\n",
    "\n",
    "responses = []\n",
    "image_idx = 0\n",
    "last_request = False\n",
    "user_data = UserData()\n",
    "sent_count = 0\n",
    "pbar_total = len(frames)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=pbar_total) as pbar:\n",
    "    while not last_request:\n",
    "        batched_image_data = None\n",
    "\n",
    "        repeated_image_data = []\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            frame = frames[image_idx]\n",
    "            \n",
    "            img = frame._load_img()\n",
    "            repeated_image_data.append(img)\n",
    "            \n",
    "            image_idx = (image_idx + 1) % len(frames)\n",
    "            if image_idx == 0:\n",
    "                last_request = True\n",
    "\n",
    "        if max_batch_size > 0:\n",
    "            batched_image_data = np.stack(repeated_image_data, axis=0)\n",
    "        else:\n",
    "            batched_image_data = repeated_image_data[0]\n",
    "\n",
    "        # Send request\n",
    "        try:\n",
    "            req_gen_args = [batched_image_data, triton_model.input_names,\n",
    "                triton_model.output_names, triton_model.triton_dtype,\n",
    "                protocol.lower()]\n",
    "            req_generator = requestGenerator(*req_gen_args)\n",
    "            for inputs, outputs in req_generator:\n",
    "                sent_count += 1\n",
    "\n",
    "                triton_client.async_infer(\n",
    "                    model_name,\n",
    "                    inputs,\n",
    "                    partial(completion_callback, user_data),\n",
    "                    request_id=str(sent_count),\n",
    "                    model_version=model_version,\n",
    "                    outputs=outputs)\n",
    "\n",
    "        except InferenceServerException as e:\n",
    "            print(\"inference failed: \" + str(e))\n",
    "            sys.exit(1)\n",
    "        \n",
    "        pbar.update(batch_size)\n",
    "    \n",
    "    processed_count = 0\n",
    "    while processed_count < sent_count:\n",
    "        (results, error) = user_data._completed_requests.get()\n",
    "        processed_count += 1\n",
    "        if error is not None:\n",
    "            print(\"inference failed: \" + str(error))\n",
    "            sys.exit(1)\n",
    "        responses.append(results)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Average latency: ~{} seconds\".format((end_time - start_time) / sent_count))\n",
    "print(\"Average throughput: ~{} examples / second\".format(batch_size * sent_count / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8e78d-dd39-48e9-8abf-f18dc2f9a1db",
   "metadata": {},
   "source": [
    "As you can see, the gain in performance is quite significant, and considering the very small changes we made to the pipeline, it was definitely worth it!\n",
    "\n",
    "Now we pass the responses to the postprocessor that renders images with bounding boxes and show them to make sure nothing has changed compared to the http inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e0b7d-7833-4eb2-807d-c8d2fe329529",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# ~~~~~~~ COMPLETE THIS SECTION ~~~~~~~ #############\n",
    "print(\"Gathering responses from the server and post-processing the inferenced outputs \\n\")\n",
    "\n",
    "args_postprocessor = [\n",
    "    #\n",
    "]\n",
    "\n",
    "postprocessor =\n",
    "###################### ~~~~~~~ END ~~~~~~~ ######################\n",
    "\n",
    "processed_request = 0\n",
    "with tqdm(total=len(frames)) as pbar:\n",
    "    while processed_request < sent_count:\n",
    "        response = responses[processed_request]\n",
    "\n",
    "        this_id = response.get_response().id\n",
    "\n",
    "        postprocessor.apply(\n",
    "            response, this_id, render=True\n",
    "        )\n",
    "        processed_request += 1\n",
    "        pbar.update(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3066d09-0577-4511-9b6e-05b34a250ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images\n",
    "OUTPUT_PATH = os.path.join(output_path, 'infer_images')\n",
    "COLS = 3 # number of columns in the visualizer grid\n",
    "IMAGES = 9 # number of images to visualize\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa35306-a371-4c7f-b13e-4e4d8c4c07d9",
   "metadata": {},
   "source": [
    "In this notebook, you have reviewed some concepts related to deployment using Triton Inference Server. Congratulations, with this you have also finished the challenges we have prepared, we hope they have been helpful in establishing the main concepts.\n",
    "\n",
    "## Other bootcamps\n",
    "\n",
    "The contents of this bootcamp originate from the [OpenHackathons Github](https://github.com/openhackathons-org). You are welcome to visit the page and search for other material that may interest you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e115e7f-861f-436c-9ee0-c6dd865a008c",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Licensing\n",
    "\n",
    "Copyright © 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be58d4a7-01b3-4f4b-9489-c8a631839d2d",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"6.Challenge_DeepStream.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 34%; text-align: center;\">\n",
    "        <a href=\"1.Data_labeling_and_preprocessing.ipynb\">1</a>\n",
    "        <a href=\"2.Object_detection_using_TAO_YOLOv4.ipynb\">2</a>\n",
    "        <a href=\"3.Model_deployment_with_Triton_Inference_Server.ipynb\">3</a>\n",
    "        <a href=\"4.Model_deployment_with_DeepStream.ipynb\">4</a>\n",
    "        <a href=\"5.Measure_object_size_using_OpenCV.ipynb\">5</a>\n",
    "        <a href=\"6.Challenge_DeepStream.ipynb\">6</a>\n",
    "        <a >7</a>\n",
    "    </span>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<p> <center> <a href=\"../Start_here.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
