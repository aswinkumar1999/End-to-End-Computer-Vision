{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a07057df-8651-4f1a-a63e-d89bb90a348e",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../Start_here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"4.Model_deployment_with_DeepStream.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 34%; text-align: center;\">\n",
    "        <a href=\"1.Data_labeling_and_preprocessing.ipynb\">1</a>\n",
    "        <a href=\"2.Object_detection_using_TAO_YOLOv4.ipynb\">2</a>\n",
    "        <a href=\"3.Model_deployment_with_Triton_Inference_Server.ipynb\">3</a>\n",
    "        <a href=\"4.Model_deployment_with_DeepStream.ipynb\">4</a>\n",
    "        <a >5</a>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662cbca0-c558-462e-8693-76adc63ee401",
   "metadata": {},
   "source": [
    "# Measure object size using OpenCV\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c0e08c-9499-43d6-85f1-113ef1aade1c",
   "metadata": {},
   "source": [
    "**The goal of this notebook is to make you understand how to:**\n",
    "\n",
    "- Get started with OpenCV basics like loading images, converting colors, and more\n",
    "- Use OpenCV for tasks like edge and color detection\n",
    "- Get information about the contours from the edge/color masks\n",
    "- Automatically estimate the size of objects\n",
    "- Process and render a live video stream\n",
    "\n",
    "**Contents of this notebook:**\n",
    "\n",
    "- [Introduction to OpenCV](#Introduction-to-OpenCV)\n",
    "- [Edge and color detection](#Edge-and-color-detection)\n",
    "    - [Canny edge detection algorithm](#Canny-edge-detection-algorithm)\n",
    "    - [HSV color thresholding](#HSV-color-thresholding)\n",
    "- [Size detection with contours](#Size-detection-with-contours) \n",
    "- [Get object size statistics](#Get-object-size-statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14469cf-a3f8-475d-b85e-453e9590f669",
   "metadata": {},
   "source": [
    "## Introduction to OpenCV\n",
    "\n",
    "Deep learning is a great tool and delivers stunning results, but it doesn't necessarily represent the best choice for every task. In situations where it is enough to look at the colors or shapes of objects to get the desired results, avoiding feeding images through a neural network and working at the pixel level can be much faster and more convenient. Not only does this not require an annotated dataset and training resources, but it also requires less effort to achieve comparable performance. And this is where OpenCV comes in!\n",
    "\n",
    "[OpenCV](https://opencv.org/) (Open Source Computer Vision Library) is an open-source, modular library that includes several hundred computer vision algorithms intended primarily for real-time applications. OpenCV is written in C++ and its primary interface is in C++ but provides bindings in other programming languages including Python and Java. The API for these interfaces can be found in the online documentation. \n",
    "\n",
    "OpenCV natively delivers features for a myriad of use cases, from simple to very complex. Among the simpler ones, we think for example of tasks such as detection of edges, colors, lines, circles, and template matching. Among the more complex ones, we find instead camera calibration, real-time pose estimation, and the possibility of inferring neural networks.\n",
    "\n",
    "<img src=\"images/res_mario.jpg\" width=\"720\">\n",
    "<div style=\"font-size:11px\">Source: https://docs.opencv.org/</div><br>\n",
    "\n",
    "In this notebook, we will explore how to use OpenCV to measure the size of our fruits and sort them into three categories based on that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5a66b0-c944-4118-a32e-28246184b63c",
   "metadata": {},
   "source": [
    "## Edge and color detection\n",
    "\n",
    "OpenCV offers great functionalities for tasks such as edge and color detection. As soon as one of these is performed, getting the size of an object is immediate as they are strongly correlated. Let's see them in practice in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597a9eb8-5c27-49e8-a4bd-0a22bfee32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "from imutils import perspective, contours\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "sample_image = \"../data/testing/image_2/0072.png\"\n",
    "\n",
    "# load the sample image with OpenCV\n",
    "image = cv2.imread(sample_image)\n",
    "\n",
    "# OpenCV loads images in BGR. Convert to RGB to view with plt\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c26b08-1071-4c3a-a581-d9edb5bf055b",
   "metadata": {},
   "source": [
    "### Canny edge detection algorithm\n",
    "\n",
    "In this cell, we perform edge detection using the Canny algorithm. It is a multi-stage algorithm that includes noise reduction, finding the intensity gradient of the image, non-maximum suppression, and hysteresis thresholding. We follow it with two rounds of dilation and erosion to close the gaps between object edges and render a more clear boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262da196-9d08-4024-a059-8442585e2d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to grayscale, and blur it slightly\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "# perform edge detection, then perform dilation + erosion to\n",
    "# close gaps in between object edges\n",
    "edged = cv2.Canny(gray, 60, 120)\n",
    "edged = cv2.dilate(edged, None, iterations=2)\n",
    "edged = cv2.erode(edged, None, iterations=2)\n",
    "\n",
    "plt.imshow(edged, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a2fec9-6973-49f5-969f-6427b672895c",
   "metadata": {},
   "source": [
    "### HSV color thresholding\n",
    "\n",
    "Another solution that leads to the same result uses threshold-based color filtering in the HSV (for hue, saturation, value) color space. In this case, we do not detect edges but entire regions whose HSV values lie in one or more ranges, leading to a filled mask representing the position of the objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f017abd-ef6a-4385-ae7a-5a08b26278d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurred = cv2.GaussianBlur(image, (3, 3), 0)\n",
    "# convert to hsv color space\n",
    "hsv = cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# color thresholding\n",
    "# lower boundary red color range values: Hue (0 - 15)\n",
    "lower1 = np.array([0, 50, 20])\n",
    "upper1 = np.array([15, 255, 255])\n",
    "# using inRange function to get only red colors\n",
    "lower_mask = cv2.inRange(hsv, lower1, upper1) \n",
    "# upper boundary red color range values: Hue (170 - 180)\n",
    "lower2 = np.array([170, 50, 20])\n",
    "upper2 = np.array([180, 255, 255])\n",
    "upper_mask = cv2.inRange(hsv, lower2, upper2)\n",
    "\n",
    "# merge the masks\n",
    "mask = lower_mask | upper_mask\n",
    "# remove noise\n",
    "mask = cv2.erode(mask, None, iterations=2)\n",
    "mask = cv2.dilate(mask, None, iterations=2)\n",
    "\n",
    "plt.imshow(mask, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865225ba-2ec8-43e8-bc2c-f63c19821dae",
   "metadata": {},
   "source": [
    "## Size detection with contours\n",
    "\n",
    "In both cases, we now have the ability to find contours in the edge or color mask which define the size of the object. In particular, we keep the largest one which should match the only fruit in the image.\n",
    "\n",
    "We also initialize a `pixels_per_metric` variable which stores how many pixels there are in a unit of measurement, say centimeters. This variable should be set by taking an object with a known size and seeing its pixel size in the camera footage. For a more robust system, you could also include a reference object in each frame, such as an ArUco marker, and use it to calibrate the camera from time to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49793e4d-5e1f-4464-9139-73743c2d4222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find contours in the edge map\n",
    "cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,\n",
    "                        cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# or in the HSV color mask\n",
    "# cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL,\n",
    "#     cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "cnts = imutils.grab_contours(cnts)\n",
    "\n",
    "# sort the contours, keep the largest, and set 'pixels_per_metric' calibration variable\n",
    "cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
    "c = cnts[0]\n",
    "pixels_per_metric = 38 # pixels per cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d39ec76-c051-4b62-b215-a1177dcbf2aa",
   "metadata": {},
   "source": [
    "As a final step, we can obtain the bounding box with the minimal area of the contour and access its dimensions. This gives an idea of the size of the object and is particularly accurate if it is rectangular in shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789a1175-9376-44d5-a36b-9bc24ff2ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def midpoint(ptA, ptB):\n",
    "    return ((ptA[0] + ptB[0]) * 0.5, (ptA[1] + ptB[1]) * 0.5)\n",
    "\n",
    "# compute the rotated bounding box of the contour\n",
    "orig = image.copy()\n",
    "box = cv2.minAreaRect(c)\n",
    "box = cv2.cv.BoxPoints(box) if imutils.is_cv2() else cv2.boxPoints(box)\n",
    "box = np.array(box, dtype=\"int\")\n",
    "\n",
    "# order the points in the contour such that they appear\n",
    "# in top-left, top-right, bottom-right, and bottom-left\n",
    "# order, then draw the outline of the rotated bounding box\n",
    "box = perspective.order_points(box)\n",
    "cv2.drawContours(orig, [box.astype(\"int\")], -1, (0, 255, 0), 2)\n",
    "\n",
    "# loop over the original points and draw them\n",
    "for (x, y) in box:\n",
    "    cv2.circle(orig, (int(x), int(y)), 5, (0, 0, 255), -1)\n",
    "\n",
    "# unpack the ordered bounding box, then compute the midpoint\n",
    "# between the top-left and top-right coordinates, followed by\n",
    "# the midpoint between bottom-left and bottom-right coordinates\n",
    "(tl, tr, br, bl) = box\n",
    "(tltrX, tltrY) = midpoint(tl, tr)\n",
    "(blbrX, blbrY) = midpoint(bl, br)\n",
    "\n",
    "# compute the midpoint between the top-left and bottom-left points,\n",
    "# followed by the midpoint between the top-right and bottom-right\n",
    "(tlblX, tlblY) = midpoint(tl, bl)\n",
    "(trbrX, trbrY) = midpoint(tr, br)\n",
    "\n",
    "# draw the midpoints on the image\n",
    "cv2.circle(orig, (int(tltrX), int(tltrY)), 5, (255, 0, 0), -1)\n",
    "cv2.circle(orig, (int(blbrX), int(blbrY)), 5, (255, 0, 0), -1)\n",
    "cv2.circle(orig, (int(tlblX), int(tlblY)), 5, (255, 0, 0), -1)\n",
    "cv2.circle(orig, (int(trbrX), int(trbrY)), 5, (255, 0, 0), -1)\n",
    "\n",
    "# draw lines between the midpoints\n",
    "cv2.line(orig, (int(tltrX), int(tltrY)), (int(blbrX), int(blbrY)),\n",
    "    (255, 0, 255), 2)\n",
    "cv2.line(orig, (int(tlblX), int(tlblY)), (int(trbrX), int(trbrY)),\n",
    "    (255, 0, 255), 2)\n",
    "\n",
    "# compute the Euclidean distance between the midpoints\n",
    "dA = dist.euclidean((tltrX, tltrY), (blbrX, blbrY))\n",
    "dB = dist.euclidean((tlblX, tlblY), (trbrX, trbrY))\n",
    "\n",
    "# compute the size of the object\n",
    "dimA = dA / pixels_per_metric\n",
    "dimB = dB / pixels_per_metric\n",
    "print(f\"Size of the bounding rectangle: {dimA:.1f}cm x {dimB:.1f}cm \\n\")\n",
    "\n",
    "# draw the object sizes on the image\n",
    "cv2.putText(orig, \"{:.1f}cm\".format(dimA),\n",
    "    (int(tltrX - 15), int(tltrY - 10)), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    0.65, (0, 0, 0), 2)\n",
    "cv2.putText(orig, \"{:.1f}cm\".format(dimB),\n",
    "    (int(trbrX + 5), int(trbrY)), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    0.65, (0, 0, 0), 2)\n",
    "\n",
    "# show the output image\n",
    "plt.imshow(cv2.cvtColor(orig, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f6407-c8f5-4d6e-9371-998476326b73",
   "metadata": {},
   "source": [
    "If the object has spherical symmetry, we can use the minimum enclosing circle instead and use the diameter as a better measure of its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865e93b-a393-42b8-af55-c0c35f68f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the minimum enclosing circle of the contour\n",
    "orig = image.copy()\n",
    "(x, y), radius = cv2.minEnclosingCircle(c)\n",
    "\n",
    "# draw the circle\n",
    "cv2.circle(orig, (int(x), int(y)), int(radius), (0, 255, 0), 2)\n",
    "\n",
    "# draw a diameter and end points\n",
    "cv2.line(orig, (int(x - radius), int(y)), (int(x + radius), int(y)),\n",
    "    (255, 0, 255), 2)\n",
    "cv2.circle(orig, (int(x - radius), int(y)), 5, (255, 0, 0), -1)\n",
    "cv2.circle(orig, (int(x + radius), int(y)), 5, (255, 0, 0), -1)\n",
    "\n",
    "# draw the center\n",
    "cv2.circle(orig, (int(x), int(y)), 5, (0, 0, 255), -1)\n",
    "\n",
    "# compute the size of the object\n",
    "dimR = radius / pixels_per_metric\n",
    "print(f\"Diameter of the object: {2 * dimR:.1f}cm\")\n",
    "\n",
    "# draw the object sizes on the image\n",
    "cv2.putText(orig, \"{:.1f}cm\".format(2 * dimR),\n",
    "    (int(x - 15), int(y - 10)), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    0.65, (0, 0, 0), 2)\n",
    "\n",
    "# show the output image\n",
    "plt.imshow(cv2.cvtColor(orig, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419db00d-e738-4f6b-a154-835dd7ee14b9",
   "metadata": {},
   "source": [
    "Great, we now have a way to measure the size of an object using OpenCV! If all the images were taken by the same camera in the same location, or if there was a reference object in each image to compare with, we could easily estimate the sizes of a series of objects using OpenCV and compare them. Our fruit dataset doesn't have this property, but we will still assume all the images of the oranges were taken from the same camera and classify the diameter of fruits, in particular oranges, into three different categories: `small`, `medium`, and `large`.\n",
    "\n",
    "## Get object size statistics\n",
    "\n",
    "Let's start by loading a module that takes an image as input and returns the size of an object as output. In particular, we are interested in detecting the size of oranges and automating the retrieval of dataset statistics. We will use the HSV mask to detect the color orange and the minimum enclosing circle strategy since we expect relatively spherical fruits. Next, we will apply this function to some images of oranges and get a size distribution for the fresh oranges in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08c0662-7868-4eb0-9014-01ddc5434f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../source_code/N5\")\n",
    "from calc_object_size import calc_object_size\n",
    "import os\n",
    "\n",
    "image_dir = \"../source_code/N5/oranges\"\n",
    "output_dir = \"../source_code/N5/output\"\n",
    "valid_image_ext = ['.jpg', '.png', '.jpeg']\n",
    "\n",
    "!rm -rf $output_dir\n",
    "!mkdir $output_dir\n",
    "\n",
    "sizes = []\n",
    "for image in os.listdir(image_dir):\n",
    "    if os.path.splitext(image)[1].lower() in valid_image_ext:\n",
    "        img_path = os.path.join(image_dir, image)\n",
    "        output_path = os.path.join(output_dir, image)\n",
    "        sizes.append(calc_object_size(img_path, output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd3282-e46e-4b4b-9855-aca2f84c6a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid visualizer\n",
    "from math import ceil\n",
    "      \n",
    "def visualize_images(image_dir, num_cols=4, num_images=10):\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(image_dir, image) for image in os.listdir(image_dir) \n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img)\n",
    "        \n",
    "# Visualizing the sample images\n",
    "OUTPUT_PATH = os.path.join(\"../source_code/N5\", \"output\")\n",
    "COLS = 3 # number of columns in the visualizer grid\n",
    "IMAGES = 9 # number of images to visualize\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dbeb81-b5e4-4ec2-9b9f-872c563aae6a",
   "metadata": {},
   "source": [
    "Now let's draw a histogram to see how the dimensions are distributed in our dataset and highlight the threshold for our division into three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d554ac96-6a47-4ea2-ac38-4c1406e10014",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,5])\n",
    "plt.hist(sizes, bins=len(sizes)//2, color='orange', edgecolor='k', alpha=0.6)\n",
    "plt.axvline(np.quantile(sizes, 1/3), color='g', linestyle='dashed', lw=3)\n",
    "plt.axvline(np.quantile(sizes, 2/3), color='g', linestyle='dashed', lw=3)\n",
    "plt.title(\"Histogram of the size of oranges: tertiles in green\")\n",
    "plt.xlabel(\"cm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e0c90-20a4-48d4-bb62-74442f85b4ad",
   "metadata": {},
   "source": [
    "Now that we have collected this data, we can classify oranges into three dimensions - `small`, `medium`, and `large` - continuously on a live video stream. Sorting will take place according to the following rule: if the size of an orange is in the first tertile of the size distribution, we will say that the orange is small; if it exceeds the second threshold, then it is large; otherwise, it is classified as medium.\n",
    "\n",
    "Please note that these thresholds could perfectly be predefined values as well, and the previous part could be skipped if data is not yet available or the sizes are set by other standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ffea0-83c6-4180-a2b8-1ddac8c46dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = \"../source_code/N5/oranges.mp4\" # Source: https://depositphotos.com\n",
    "output_video = \"../source_code/N5/out.avi\"\n",
    "\n",
    "# thresholds\n",
    "q1 = np.quantile(sizes, 1/3)\n",
    "q2 = np.quantile(sizes, 2/3)\n",
    "\n",
    "print(f\"Using thresholds {q1:.1f}cm and {q2:.1f}cm ...\")\n",
    "pixels_per_metric = 28\n",
    "\n",
    "\n",
    "# load a video stream from a file\n",
    "cap = cv2.VideoCapture(input_video)\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# get the video size\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4)) \n",
    "size = (frame_width, frame_height)\n",
    "fps = 20\n",
    "   \n",
    "# VideoWriter object will save a processed frame of the \n",
    "# above video in the output video file\n",
    "out = cv2.VideoWriter(output_video, \n",
    "    cv2.VideoWriter_fourcc(*'XVID'), fps, size)\n",
    "  \n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read() # ret checks return at each frame\n",
    "    \n",
    "    if ret:\n",
    "        blurred = cv2.GaussianBlur(frame, (3, 3), 0)\n",
    "        # convert to hsv color space\n",
    "        hsv = cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # color thresholding\n",
    "        # orange color range values: Hue (5 - 25)\n",
    "        lower = np.array([5, 140, 190])\n",
    "        upper = np.array([25, 255, 255])\n",
    "        # using inRange function to get only orange colors\n",
    "        mask = cv2.inRange(hsv, lower, upper)\n",
    "        # remove noise\n",
    "        mask = cv2.erode(mask, None, iterations=2)\n",
    "        mask = cv2.dilate(mask, None, iterations=2)\n",
    "\n",
    "        # find contours in the edge map\n",
    "        cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL,\n",
    "            cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = imutils.grab_contours(cnts)\n",
    "        \n",
    "        orig = frame.copy()\n",
    "        # loop over the contours individually\n",
    "        for c in cnts:\n",
    "            # if the contour is not sufficiently large, ignore it\n",
    "            if cv2.contourArea(c) < 10000:\n",
    "                continue\n",
    "\n",
    "            # compute the minimum enclosing circle of the contour\n",
    "            (x, y), radius = cv2.minEnclosingCircle(c)\n",
    "            \n",
    "            # compute the size of the object\n",
    "            dimR = radius / pixels_per_metric\n",
    "            \n",
    "            color = (0, 255, 0) # medium -> green\n",
    "            category = \"medium\"\n",
    "            if 2 * dimR < q1: \n",
    "                color = (255, 255, 0) # small -> cyan\n",
    "                category = \"small\"\n",
    "            if 2 * dimR > q2:\n",
    "                color = (0, 0, 255) # large -> red\n",
    "                category = \"large\"\n",
    "                \n",
    "            # draw the circle\n",
    "            cv2.circle(orig, (int(x), int(y)), int(radius), color, 2)\n",
    "\n",
    "            # draw a diameter and end points\n",
    "            cv2.line(orig, (int(x - radius), int(y)), (int(x + radius), int(y)),\n",
    "                (255, 0, 255), 2)\n",
    "            cv2.circle(orig, (int(x - radius), int(y)), 5, (255, 0, 0), -1)\n",
    "            cv2.circle(orig, (int(x + radius), int(y)), 5, (255, 0, 0), -1)\n",
    "\n",
    "            # draw the center\n",
    "            cv2.circle(orig, (int(x), int(y)), 5, (0, 0, 255), -1)\n",
    "\n",
    "            # draw the object sizes on the image\n",
    "            cv2.putText(orig, \"{:.1f}cm\".format(2 * dimR),\n",
    "                (int(x - 15), int(y - 10)), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.65, (0, 0, 0), 2)\n",
    "            \n",
    "            cv2.putText(orig, f\"{category}\",\n",
    "                (int(x - 15), int(y + 20)), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.65, (0, 0, 0), 2)\n",
    " \n",
    "        # output the frame\n",
    "        out.write(orig)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "    \n",
    "# release input and output\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d06b63-7a00-42fb-919f-e3a1f64c66c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert video profile to be compatible with Jupyter notebook\n",
    "!ffmpeg -loglevel panic -y -an -i ../source_code/N5/out.avi -vcodec libx264 -pix_fmt yuv420p -profile:v baseline -level 3 ../source_code/N5/output.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9ea0b6-23bf-466b-83e9-07b9eba51915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the output\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    " <video width=\"640\" height=\"480\" controls>\n",
    " <source src=\"../source_code/N5/output.mp4\"\n",
    " </video>\n",
    "\"\"\".format())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf245cd-5a97-4065-8e9d-8052f61c764e",
   "metadata": {},
   "source": [
    "Note that here we performed this process in a separate notebook for educational purposes using only OpenCV functions, but in a true computer vision application this type of OpenCV post-processing may need to be integrated into a more complex DeepStream or Triton Inference Server pipeline, seen in the previous two notebooks. In the first case, you can find the implementation of a reference application that includes OpenCV in the [DeepStream Python Apps](https://github.com/NVIDIA-AI-IOT/deepstream_python_apps) GitHub repository, while in the second use case an OpenCV routine can be easily added to the function that processes the result from the server and used to get additional insights from the images.\n",
    "\n",
    "Congratulations, you have successfully completed the **end-to-end computer vision** bootcamp! With this material, you now have a broader idea of what it takes to bring computer vision applications to life just starting from unlabeled data. You've also worked with multiple NVIDIA SDKs and followed a development flow that generalizes to many other use cases. Thank you for participating!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f98cca-3c33-48b2-946b-fddc18b3a5d6",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## References\n",
    "\n",
    "- [1] *https://pyimagesearch.com/2016/03/28/measuring-size-of-objects-in-an-image-with-opencv*\n",
    "- [2] *https://learnopencv.com/read-write-and-display-a-video-using-opencv-cpp-python*\n",
    "\n",
    "## Licensing\n",
    "\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dced65cc-b559-485b-969c-106488a18fc9",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"4.Model_deployment_with_DeepStream.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 34%; text-align: center;\">\n",
    "        <a href=\"1.Data_labeling_and_preprocessing.ipynb\">1</a>\n",
    "        <a href=\"2.Object_detection_using_TAO_YOLOv4.ipynb\">2</a>\n",
    "        <a href=\"3.Model_deployment_with_Triton_Inference_Server.ipynb\">3</a>\n",
    "        <a href=\"4.Model_deployment_with_DeepStream.ipynb\">4</a>\n",
    "        <a >5</a>\n",
    "    </span>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<p> <center> <a href=\"../Start_here.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
