{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../Start_here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    "<div>\n",
    "    <span style=\"float: left; width: 52%; text-align: right;\">\n",
    "        <a >1</a>\n",
    "        <a href=\"2.Object_detection_using_TAO_YOLOv4.ipynb\">2</a>\n",
    "        <a href=\"3.Model_deployment_with_Triton_Inference_Server.ipynb\">3</a>\n",
    "        <a href=\"4.Model_deployment_with_DeepStream.ipynb\">4</a>\n",
    "        <a href=\"5.Measure_object_size_using_OpenCV.ipynb\">5</a>\n",
    "        <a href=\"6.Challenge_DeepStream.ipynb\">6</a>\n",
    "        <a href=\"7.Challenge_Triton.ipynb\">7</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 48%; text-align: right;\"><a href=\"2.Object_detection_using_TAO_YOLOv4.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data labeling and preprocessing\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of this notebook is to make you understand how to:**\n",
    "\n",
    "- Label data for object detection applications\n",
    "- Convert a dataset into KITTI format\n",
    "\n",
    "**Contents of this notebook:**\n",
    "\n",
    "- [Custom data labeling](#Custom-data-labeling)\n",
    "    - [Labeling with Label Studio](#Labeling-with-Label-Studio)\n",
    "    - [Labeling with Yolo Mark](#Labeling-with-Yolo-Mark)\n",
    "- [Download data for the lab](#Download-data-for-the-lab)\n",
    "- [Conversion to KITTI format](#Conversion-to-KITTI-format)\n",
    "    - [Load the dataset](#Load-the-dataset)\n",
    "    - [Export to KITTI](#Export-to-KITTI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Custom data labeling\n",
    "\n",
    "Training a deep learning model for an object detection task requires a meaningful amount of annotated data. A dataset for a specific domain application may not be available often or if it is, chances are it may not be labeled or adequate in size. In this notebook, we show how to annotate a custom dataset with bounding boxes and convert it into KITTI file format, useful to expand the number of samples with offline data augmentation or to train a model with transfer learning.\n",
    "\n",
    "<img src=\"images/prep_pipeline.png\" width=\"720\">\n",
    "\n",
    "We present two tools for data labeling operations:\n",
    "- Label Studio\n",
    "- Yolo Mark\n",
    "\n",
    "We recommend using Label Studio because of the more intuitive user interface and a better overall labeling experience.\n",
    "\n",
    "### Labeling with Label Studio\n",
    "\n",
    "[Label Studio](https://labelstud.io/) is an open-source, flexible, quickly installable data labeling tool with a very convenient user interface. The tool natively comes with a Python module available to install via the pip package manager, but can also be installed in alternative ways, all available [here](https://labelstud.io/guide/install.html), so feel free to pick the one you are most comfortable with.\n",
    "\n",
    "To get started with the Python module, open a terminal window in your preferred environment (ideally, create a fresh virtual one) and run the command `pip install -U label-studio`. Once installed, start the server with the command `label-studio`. This will automatically open a user interface on the default web browser on port 8080, accessible at `http://localhost:8080` if you are working on your local machine, unless another port is specified.\n",
    "\n",
    "To proceed, follow these steps and visual explanations:\n",
    "- Sign up with an email address and create a password (that these credentials are stored locally on the Label Studio server and can be whatever you prefer).\n",
    "<img src=\"images/label_studio_1.png\" width=\"720\">\n",
    "\n",
    "- Create a new project.\n",
    "<img src=\"images/label_studio_2.png\" width=\"720\">\n",
    "\n",
    "- Give it a title and optionally a brief description.\n",
    "<img src=\"images/label_studio_3.png\" width=\"720\">\n",
    "\n",
    "- Drag and drop images to upload.\n",
    "<img src=\"images/label_studio_4.png\" width=\"720\">\n",
    "\n",
    "- Select an object detection task with bounding boxes.\n",
    "<img src=\"images/label_studio_5.png\" width=\"720\">\n",
    "\n",
    "- Set the class names.\n",
    "<img src=\"images/label_studio_6.png\" width=\"720\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan on tagging a significant amount of data, you will likely need to separate it into multiple chunks to avoid hitting the per-project memory limit.\n",
    "\n",
    "Once the previous steps are completed, you can start with the labeling process. From the project menu, click on `Label All Tasks` at the top.\n",
    "\n",
    "<img src=\"images/label_studio_7.png\" width=\"720\">\n",
    "\n",
    "Then, for every image, do the following operations:\n",
    "- Select an appropriate class.\n",
    "- Draw all the bounding boxes for that class.\n",
    "- Repeat for other classes.\n",
    "- Click `Submit`.\n",
    "\n",
    "<img src=\"images/label_studio_8.png\" width=\"720\">\n",
    "\n",
    "This will automatically load the next image until there are no images left. While labeling, you can stop at any time and when you resume, you will continue exactly where you left off.\n",
    "\n",
    "<img src=\"images/label_studio_9.png\" width=\"720\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as you have completed the labeling activity, either because you have run out of images or because you are satisfied with how many you have, you can go back to the home page of the project, apply filters to the annotations, and export them by clicking on `Export`. Make sure to scroll down and select the YOLO format when you do so.\n",
    "\n",
    "<img src=\"images/label_studio_10.png\" width=\"720\">\n",
    "\n",
    "For more in-depth information and an additional visual explanation of the previous steps, explore this [dedicated tutorial](https://labelstud.io/blog/Quickly-Create-Datasets-for-Training-YOLO-Object-Detection.html) on how to label images for YOLO applications on the Label Studio blog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exported data has a similar structure to this one by default, after unzipping the downloaded file:\n",
    "```\n",
    "project-1-at-2022-09-20-15-20-f6c05363.zip\n",
    "    notes.json\n",
    "    classes.txt\n",
    "    labels\n",
    "        image_filename1.txt\n",
    "        image_filename2.txt\n",
    "        image_filename3.txt\n",
    "        ...\n",
    "    images\n",
    "        image_filename1.<ext>\n",
    "        image_filename2.<ext>\n",
    "        image_filename3.<ext>\n",
    "        ...\n",
    "```\n",
    "<img src=\"images/label_studio_11.png\" width=\"720\">\n",
    "\n",
    "The TXT files in the `labels` folder are space-delimited files where each row corresponds to an object in the image with the same name in the `images` folder, in the standard YOLO format:\n",
    "```\n",
    "<target> <x-center> <y-center> <width> <height> <confidence>\n",
    "```\n",
    "<img src=\"images/yolo_label.png\" width=\"720\">\n",
    "\n",
    "where `<target>` is the zero-based integer index of the object class label from `classes.txt`, the bounding box coordinates are expressed as relative coordinates in `[0, 1] x [0, 1]`, and `<confidence>` is an optional detection confidence in `[0, 1]`, left blank by Label Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling with Yolo Mark\n",
    "\n",
    "Another popular data labeling tool is [Yolo Mark](https://github.com/AlexeyAB/Yolo_mark), a Windows and Linux GUI for marking bounded boxes of objects in images for training Yolo. Its use is not as straightforward as Label Studio, as it needs to be compiled from source and does not come with a Python module, but is still as an option to consider for a project.\n",
    "\n",
    "In order to use Yolo Mark, [download](https://github.com/AlexeyAB/Yolo_mark) the repository from GitHub and follow the instructions in the README file to get the executable program, depending on your operating system. Note that a working installation of [OpenCV](https://opencv.org/) is required to run the program successfully. If you are a Windows user you might consider a tool like [MS Visual Studio](https://visualstudio.microsoft.com/vs/) to compile the project, while for Linux users, you will just need to type the commands `cmake .` and then `make` after moving into the project directory.\n",
    "\n",
    "At this point, to use the tool to label your custom images, place them in the `x64/Release/data/img` directory, change the number of classes in `x64/Release/data/obj.data` as well as the class names in `x64/Release/data/obj.names`, and run `x64/Release/yolo_mark.cmd` on Windows or `./linux_mark.sh` on Linux to start labeling.\n",
    "\n",
    "<img src=\"images/yolo_mark.png\" width=\"720\">\n",
    "\n",
    "The resulting YOLO dataset in `x64/Release/data` will have the following structure:\n",
    "```\n",
    "data\n",
    "    obj.data\n",
    "    obj.names\n",
    "    train.txt\n",
    "    img\n",
    "        image_filename1.<ext>\n",
    "        image_filename1.txt\n",
    "        image_filename2.<ext>\n",
    "        image_filename2.txt\n",
    "        image_filename3.<ext>\n",
    "        image_filename3.txt\n",
    "        ...\n",
    "```            \n",
    "with images and corresponding labels in the same folder, `obj.names` with the class names, and a `train.txt` file with the paths to the labeled images. The format of the TXT annotation files in the `img` folder is the same YOLO format as described before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data for the lab\n",
    "\n",
    "In this lab, we will provide you with a labeled version of a dataset containing three types of fruit - `apples`, `bananas`, and `oranges` - each fresh or rotten, for a total of six classes. The dataset was labeled using Label Studio, as explained above. The project folder has been renamed to `label-studio`. Running the following cell will make the data available in the `/workspace/data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../source_code/dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conversion to KITTI format\n",
    "\n",
    "Regardless of whether Label Studio or Yolo Mark was used, or a dataset already labeled in YOLO format was provided, conversion to KITTI format is required to experiment with the NVIDIA® TAO Toolkit in the next notebook. The KITTI format not only allows you to unleash the power of transfer learning and pre-trained models available within the TAO Toolkit but also is used to perform offline data augmentation and dramatically increase the size of the dataset.\n",
    "\n",
    "The KITTI format organizes the data directories of images and corresponding labels into a structure similar to Label Studio, namely:\n",
    "```\n",
    "dataset_dir\n",
    "    data\n",
    "        image_filename1.<ext>\n",
    "        image_filename2.<ext>\n",
    "        image_filename3.<ext>\n",
    "        ...\n",
    "    labels\n",
    "        image_filename1.txt\n",
    "        image_filename2.txt\n",
    "        image_filename3.txt\n",
    "        ...\n",
    "```  \n",
    "The main difference is that in the KITTI format the labels TXT files are space-delimited files where each row corresponds to an object and **the bounding box is stored using 15 (and optional 16th confidence) columns**. The meaning of each of the 15 required columns is described [here](https://docs.nvidia.com/tao/tao-toolkit/text/data_annotation_format.html#label-files). In particular, the first item is the object label and from the fifth to the eighth position we have the bounding box coordinates expressed in pixels **[x-top-left, y-top-left, x-bottom-right, y-bottom-right]**. Note that this is different from the YOLO format since we now use corners to identify the box and it is not resizing invariant.\n",
    "\n",
    "<img src=\"images/yolo_kitti.png\" width=\"720\">\n",
    "\n",
    "To perform the conversion between dataset formats, we will use [FiftyOne](https://voxel51.com/docs/fiftyone/), an open-source Python tool for handling computer vision datasets. FiftyOne allows loading a YOLO dataset and exporting it as KITTI in a few lines of code.\n",
    "\n",
    "### Load the dataset\n",
    "\n",
    "The generic `Dataset.from_dir()` method (documentation available [here](https://voxel51.com/docs/fiftyone/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.from_dir)) loads a dataset from disk and depending on the format, additional parameters can be passed to customize the data import. When dealing with a YOLO data format like in our case, these parameters are inherited from the [YOLOv4DatasetImporter](https://voxel51.com/docs/fiftyone/api/fiftyone.utils.yolo.html#fiftyone.utils.yolo.YOLOv4DatasetImporter) class and a customized import would require the following arguments:\n",
    "- `dataset_dir`: the dataset directory.\n",
    "- `dataset_type`: the `fiftyone.types.dataset_types.Dataset` type of the dataset.\n",
    "- `data_path`: to enable explicit control over the location of the media.\n",
    "- `labels_path`: to enable explicit control over the location of the labels.\n",
    "- `images_path`: to enable explicit control over the location of the image listing file.\n",
    "- `objects_path`: to enable explicit control over the location of the object names file.\n",
    "\n",
    "If your data stored on disk is not in YOLO format but in one of the [many common formats](https://voxel51.com/docs/fiftyone/user_guide/dataset_creation/datasets.html#supported-import-formats) supported natively by FiftyOne, then you can automatically load your data with minimal code changes in terms of additional parameters.\n",
    "\n",
    "To install the FiftyOne Python module, run `pip install fiftyone` in your preferred environment (ideally, a virtual one). In this lab, we have already installed it for you.\n",
    "\n",
    "Let's now load a YOLO dataset generated with Label Studio into FiftyOne. In this case, we have an object names file but we don't have an image listing file, so we just ignore the `images_path` argument and let FiftyOne list the data directory for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "dataset_dir = \"../data/label-studio/\"\n",
    "data_path = \"images/\"\n",
    "labels_path = \"labels/\"\n",
    "objects_path = \"classes.txt\"\n",
    "\n",
    "# Create the dataset\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    data_path=data_path,\n",
    "    labels_path=labels_path,\n",
    "    objects_path=objects_path,\n",
    "    dataset_type=fo.types.YOLOv4Dataset\n",
    ")\n",
    "\n",
    "# View summary info about the dataset\n",
    "print(dataset)\n",
    "\n",
    "# Print the first few samples in the dataset\n",
    "print(dataset.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, if we were trying to load a dataset generated with Yolo Mark into FiftyOne, saved into a folder named `yolo-mark` that isn't available for the lab, images and labels would now be in the same folder and we would have both an object names file and an image listing file. However, the `train.txt` image listing file contains paths from the executable file directory and not from the dataset home directory, so FiftyOne will not find the images unless we substitute all paths with relative paths in the form `img/image_filename.<ext>`. We can do that with some simple code that generates a new `images.txt` file with the right paths.\n",
    "```python\n",
    "# Read the file\n",
    "with open(\"../data/yolo-mark/train.txt\", \"r\") as file :\n",
    "    filedata = file.read()\n",
    "    \n",
    "# Replace the target string\n",
    "# On Linux\n",
    "filedata = filedata.replace(\"x64/Release/data/img/\", \"img/\")\n",
    "# On Windows\n",
    "#filedata = filedata.replace(\"data/img/\", \"img/\")\n",
    "\n",
    "# Write the file out again\n",
    "with open(\"../data/yolo-mark/images.txt\", \"w\") as file:\n",
    "    file.write(filedata)\n",
    "```    \n",
    "\n",
    "Alternatively, we can again ignore the `images_path` argument and let FiftyOne list all the data directory for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use a dataset labeled with Yolo Mark, you will need a yolo-mark folder to run the code below to load it into FiftyOne\n",
    "\n",
    "# dataset_dir = \"../data/yolo-mark/\"\n",
    "# data_path = \"img/\"\n",
    "# images_path = \"images.txt\"\n",
    "# objects_path = \"obj.names\"\n",
    "\n",
    "# Create the dataset\n",
    "# dataset = fo.Dataset.from_dir(\n",
    "#     dataset_dir=dataset_dir,\n",
    "#     data_path=data_path,\n",
    "#     images_path=images_path,\n",
    "#     objects_path=objects_path,\n",
    "#     dataset_type=fo.types.YOLOv4Dataset\n",
    "# )\n",
    "\n",
    "# View summary info about the dataset\n",
    "# print(dataset)\n",
    "\n",
    "# Print the first few samples in the dataset\n",
    "# print(dataset.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to KITTI\n",
    "\n",
    "Once the dataset is loaded into FiftyOne, conversion to KITTI format is immediate with an export command. The `Dataset.export()` method (documentation available [here](https://voxel51.com/docs/fiftyone/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.export)) writes the samples to disk and a customized export to KITTI format would require the following arguments:\n",
    "- `export_dir`: the dataset export directory.\n",
    "- `dataset_type`: the `fiftyone.types.dataset_types.Dataset` type of the dataset.\n",
    "- `data_path`: to enable explicit control over the location of the exported media.\n",
    "- `labels_path`: to enable explicit control over the location of the exported labels.\n",
    "\n",
    "Providing only `export_dir` and `dataset_type` would result in an export of the content to a directory following the default layout for the specified format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = \"../data/training/\"\n",
    "data_path = \"image_2/\"\n",
    "labels_path = \"label_2/\"\n",
    "\n",
    "# Export the dataset\n",
    "dataset.export(\n",
    "    export_dir=export_dir,\n",
    "    data_path=data_path,\n",
    "    labels_path=labels_path,\n",
    "    dataset_type=fo.types.KITTIDetectionDataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now view some images of our dataset before moving on to the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid visualizer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from math import ceil\n",
    "valid_image_ext = ['.jpg', '.png', '.jpeg', '.ppm']\n",
    "\n",
    "def visualize_images(img_path, num_cols=4, num_images=10):\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(img_path, image) for image in os.listdir(img_path) \n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images\n",
    "IMG_PATH = '../data/training/image_2'\n",
    "COLS = 3 # number of columns in the visualizer grid\n",
    "IMAGES = 9 # number of images to visualize\n",
    "\n",
    "visualize_images(IMG_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have seen how to label a raw dataset and export it into KITTI format. Next, we will train an object detection model using the TAO Toolkit. Please go to the next notebook by clicking on the `Next Notebook` button below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Licensing\n",
    "\n",
    "Copyright © 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div>\n",
    "    <span style=\"float: left; width: 52%; text-align: right;\">\n",
    "        <a >1</a>\n",
    "        <a href=\"2.Object_detection_using_TAO_YOLOv4.ipynb\">2</a>\n",
    "        <a href=\"3.Model_deployment_with_Triton_Inference_Server.ipynb\">3</a>\n",
    "        <a href=\"4.Model_deployment_with_DeepStream.ipynb\">4</a>\n",
    "        <a href=\"5.Measure_object_size_using_OpenCV.ipynb\">5</a>\n",
    "        <a href=\"6.Challenge_DeepStream.ipynb\">6</a>\n",
    "        <a href=\"7.Challenge_Triton.ipynb\">7</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 48%; text-align: right;\"><a href=\"2.Object_detection_using_TAO_YOLOv4.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<p> <center> <a href=\"../Start_here.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
